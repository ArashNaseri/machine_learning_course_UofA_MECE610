{"cells":[{"cell_type":"markdown","metadata":{"id":"nihiBdwKFsbR"},"source":["## HD02 ML Basics \\cite{geron2019hands}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"98YlJEOjFsbU"},"source":["### \\insertshortlecture: Goals and Content\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o-SLTALsFsbU"},"source":["#### The Objectives and Goals for this section\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oztr8Yz_FsbV"},"source":["##### ML Basics - Background Statistics                                                   :B_block:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JEpC57WlFsbV"},"source":["1.  Linear regression\n","2.  Ridge and Lasso regression\n","3.  Logistic regression (classification)\n","4.  Unsupervised Learning - k-means algorithm\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GXg-3cLmFsbW"},"source":["### Linear Least Squares\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R6e76iecFsbW"},"source":["#### Least Squares Linear Regression\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ig0YihwpFsbX"},"source":["-   should be a review\n","-   highlight concepts in:\n","    -   **frequentist statistics::** in which model parameters are assumed fixed and we do not specify prior beliefs over these parameters\n","    -   **Bayesian statistics::** which focuses on computing the posterior belief over model parameters given a prior\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_f0oPZQHFsbY"},"source":["#### Linear regression\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4PumoqZTFsbY"},"source":["-   regression with linear functions as:\n","\n","\\begin{equation}\n","    \\hat{y} = f( \\mathbf{x}) =  \\mathbf{\\theta}^T  \\mathbf{x}.\n","\\end{equation}\n","\n","-   where $ \\mathbf{\\theta}$ are the parameters of the model.\n","-   Note that the linear function above has an intercept at zero - there is no offset term.\n","-   assume throughout for linear regression that the input vector is augmented with a $1$, so that\n","\n","\\begin{equation}\n","    \\hat{y} = \\theta_0 +  \\mathbf{\\theta}^T  \\mathbf{x} =\n","    \\begin{bmatrix}\n","    \\theta_0 &  \\mathbf{\\theta}^T\n","    \\end{bmatrix}\n","    \\begin{bmatrix}\n","    1\\\\\n","     \\mathbf{x}\n","    \\end{bmatrix}.\n","\\end{equation}\n","\n","Thus when we write $ \\mathbf{x}$, simply imagine the input augmented with a 1 at the first entry. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"dg4zUTk0FsbZ"},"source":["#### Choose Mean Squared Error (MSE) Cost\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g0m4IHtuFsbZ"},"source":["-   the Mean Squared Error} (MSE) cost function is:\n","\n","\\begin{equation}\n","    \\J(\\theta) = \\frac{1}{d}\\sum_{i=1}^{d} \\left( y_i - \\hat{y} \\right)^2.\n","\\end{equation}\n","\n","-   this is the most common loss function in regression problems due to several favorable properties\n","-   minimizing this loss encodes the objective of minimizing the squared error in our prediction.\n","-   Given our linear model parameterization and this loss function, we can now fully specify our regression problem as:\n","\n","\\begin{equation}\n","\\begin{aligned}\n","& \\underset{ \\mathbf{\\theta}}{\\min}\n","& & \\frac{1}{d}\\sum_{i=1}^{d} \\left( y_i -  \\mathbf{\\theta}^T  \\mathbf{x}_i \\right)^2\n","\\end{aligned}\n","\\end{equation}\n","\n","-   rewrite as\n","\n","\\begin{equation}\n","\\begin{aligned}\n","& \\underset{ \\mathbf{\\theta}}{\\min}\n","& & \\left\\|  \\mathbf{y} - X  \\mathbf{\\theta} \\right\\|_2^2\n","\\end{aligned}\n","\\label{eq:least_squares}\n","\\end{equation}\n","\n","-   have multiplied the objective by $d$ to simplify notation\n","-   where $  \\mathbf{y}^T = [y_1, \\ldots, y_d]$ and$X^T = [ \\mathbf{x}_1, \\ldots,  \\mathbf{x}_d]$\n","    -   The matrix $ X^T $ is typically referred to as the \\textit{design matrix}.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R72lGi0-FsbZ"},"source":["#### Solve by setting the gradient to zero\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RtyHnai3Fsba"},"source":["-   Eq \\ref{eq:least_squares} that this loss is convex in the model parameters and is differentiable with continuous derivative (in class $C^1$).\n","-   know any point satisfying the first order necessary conditions for optimality is a global minimizer.\n","-   to solve this problem compute the gradient as:\n","\n","\\begin{align}\n","    \\nabla_{ \\mathbf{\\theta}}\\left\\|  \\mathbf{y} - X  \\mathbf{\\theta} \\right\\|_2^2 = 2X^T X  \\mathbf{\\theta} - 2 X^T  \\mathbf{y}\n","    \\label{eq:LS_grad}\n","\\end{align}\n","\n","-   then set the gradient to zero, resulting in:\n","\n","\\begin{equation}\n","    X^T X  \\mathbf{\\theta} = X^T  \\mathbf{y}.\n","    \\label{eq:LS_grad2}\n","\\end{equation}\n","\n","-   If $X^T X$ is invertible then the optimal set of parameters is\n","\n","\\begin{equation}\n","    \\hat{ \\mathbf{\\theta}} = (X^T X)^{-1} X^T  \\mathbf{y}.\n","    \\label{eq:norm_eq}\n","\\end{equation}\n","\n","We will write the set of parameters that solve the least squares problem in the overdetermined form (as above) as $\\hat{ \\mathbf{\\theta}}_{LS}$. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"sIB-kIFqFsba"},"source":["#### When is \\(X^T X\\) invertible?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nPwy4shdFsba"},"source":["\\begin{equation}\n","    X^T X = \\sum_{i=1}^{d}  \\mathbf{x}_i  \\mathbf{x}_i^T.\n","\\end{equation}\n","\n","-   Let $n$ denote the dimension of the input.\n","-   Then, we require $d > n$ for $X^T X$ to be full rank.\n","-   However, at least $n$ input is not a sufficient condition for $X^T X$ to be full rank;\n","    -   we also require at least $n$ linearly independent inputs.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z_Hw26qYFsba"},"source":["### Example Normal Equations\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CJCX2w6XFsbb"},"source":["#### Example is from a Diesel Engine\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RnR7LK6jFsbb"},"source":["[figures/Diesel_schematics.pdf](figures/Diesel_schematics.pdf)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0KGAnrQ8Fsbb"},"source":["#### Python setup and load libraries\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rXfmhAZPFsbc"},"source":["In this example, the Engine NOx model is modeled using different features\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"plmX2CKOFsbc","executionInfo":{"status":"ok","timestamp":1647356404009,"user_tz":360,"elapsed":513,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","import operator\n","import matplotlib.pyplot as plt\n","get_ipython().run_line_magic('matplotlib', 'inline')"]},{"cell_type":"markdown","metadata":{"id":"AHzmQ9u-Fsbd"},"source":["#### Setup an array to store\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MkTQ54XjFsbd"},"source":["-   the parameter vector $\\theta$\n","-   the columns are $\\theta = [ \\theta_0  \\theta_1  \\theta_2  \\theta_3  \\theta_4]^T_ $\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"uHE8jB9_Fsbe","executionInfo":{"status":"ok","timestamp":1647356404010,"user_tz":360,"elapsed":8,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["# max number of parameters\n","ntheta = 5\n","# maximum number of cases\n","ncases = 10\n","# counter for case number\n","nc = 0\n","# create the array of theta's\n","thetav = np.zeros((ncases, ntheta))"]},{"cell_type":"markdown","metadata":{"id":"w9w44YYLFsbe"},"source":["#### Setup an array to store errors\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2nuhBFFFFsbe"},"source":["-   the error vector is; MAE, MSE, R2, lambda\n","-   set $\\lambda = 0$ for normal regression and use for Ridge or Lasso\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"6F8TY48DFsbe","executionInfo":{"status":"ok","timestamp":1647356404010,"user_tz":360,"elapsed":6,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["nerr = 4\n","errv = np.zeros((ncases, nerr))"]},{"cell_type":"markdown","metadata":{"id":"nekiP8HqFsbe"},"source":["#### Import Engine Data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IAK_qM5kFsbf"},"source":["Stationary Diesel engine data with\n","\n","| Load [ft.lb]|Engine speed [rpm]|mf [mg/stroke]|Pr [PSI]|NOx [ppm]|\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"z8VrkGGDFsbf","executionInfo":{"status":"error","timestamp":1647356405538,"user_tz":360,"elapsed":1533,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}},"outputId":"1a2892c2-b55a-4daf-a868-22943baf9533","colab":{"base_uri":"https://localhost:8080/","height":328}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-9a472542cac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Engine_NOx.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Engine_NOx.csv'"]}],"source":["data = pd.read_csv('Engine_NOx.csv')\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"Kv5UVFY7Fsbf"},"source":["#### Plot NOx as a function of load to look at the data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3vQ9RgbPFsbg"},"source":["-   the one \\`feature' here is Load (and a constant)\n","-   so later will fit NOx = $\\text{load}\\theta_1 + \\theta_0 $\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qtb26LruFsbg","executionInfo":{"status":"aborted","timestamp":1647356404511,"user_tz":360,"elapsed":205,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.scatter(data['Load [ft.lb]'], data['NOx [ppm]'],  color='blue')\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MMzJ0V9oFsbg"},"source":["#### Split the data in training and test data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gqYb8xHvFsbg"},"source":["-   usually you want to validate your fit on data that was not used to fit the data\n","-   here randomly select 80% of the data for fitting (training) - the 80% is arbitrarily selected\n","-   the remaining 20% will be used to validate (test data)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8vn6zEXFsbg","executionInfo":{"status":"aborted","timestamp":1647356404512,"user_tz":360,"elapsed":205,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["cdf = data[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'NOx [ppm]']]\n","\n","msk = np.random.rand(len(data)) < 0.8\n","train = cdf[msk]\n","test = cdf[~msk]"]},{"cell_type":"markdown","metadata":{"id":"7SXFiurgFsbg"},"source":["#### Plot the test and training data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqeQX5DCFsbg","executionInfo":{"status":"aborted","timestamp":1647356404512,"user_tz":360,"elapsed":205,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.scatter(train['Load [ft.lb]'], train['NOx [ppm]'],  color='blue')\n","plt.scatter(test['Load [ft.lb]'], test['NOx [ppm]'],  color='red')\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mvwsOaqBFsbh"},"source":["#### Prepare the data for sklearn\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KyGRBEkFFsbh"},"source":["-   sklearn is a python library for Machine Learning\n","-   divide into test and train and set min and max: $\\frac{x-\\min(x)}{\\max(x) - \\min(x)}$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wn1DoIBNFsbh","executionInfo":{"status":"aborted","timestamp":1647356404512,"user_tz":360,"elapsed":205,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import preprocessing\n","\n","train_x = np.asanyarray(train[['Load [ft.lb]']])\n","train_y = np.asanyarray(train[['NOx [ppm]']])\n","\n","test_x = np.asanyarray(test[['Load [ft.lb]']])\n","test_y = np.asanyarray(test[['NOx [ppm]']])\n","\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)"]},{"cell_type":"markdown","metadata":{"id":"TaSdMan5Fsbh"},"source":["#### Preparing data for normal equation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lOeOG-ACFsbh"},"source":["-   set up $X$ to be a constant (ones) and Load, while $y$ is NOx\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ST1AsiFVFsbh","executionInfo":{"status":"aborted","timestamp":1647356404513,"user_tz":360,"elapsed":206,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["numsamples = X_train_minmax.shape[0]\n","# 1st row is ones and second row is x\n","Xtrain = np.c_[np.ones((numsamples,1)),X_train_minmax]\n","ytrain = train_y"]},{"cell_type":"markdown","metadata":{"id":"bGmb-jsgFsbi"},"source":["##### Solve for the parameter vector \\( \\mathbf{\\theta} \\)  Eqn \\ref{eq:norm_eq}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mFLtcPs-Fsbi"},"source":["-   solve and print coefficients\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUABeBuRFsbi","executionInfo":{"status":"aborted","timestamp":1647356404513,"user_tz":360,"elapsed":206,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["theta_lin = np.dot(np.dot(np.linalg.inv(np.dot(Xtrain.T, Xtrain) / numsamples), Xtrain.T), ytrain) / numsamples\n","print ('Normal Equations: Coefficients: ', theta_lin[1])\n","print ('Normal Equations: Intercept: ',theta_lin[0])\n","# store this case\n","nc = 0\n","thetav[nc][0] = theta_lin[0]\n","thetav[nc][1] = theta_lin[1]"]},{"cell_type":"markdown","metadata":{"id":"ZL3rl5roFsbi"},"source":["-   then plot the fit to the training data. Fit is:  NOx = $\\text{load}\\theta_1 + \\theta_0 $\n","-   this method is really never numerically used in practice\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObnFd1IhFsbi","executionInfo":{"status":"aborted","timestamp":1647356404513,"user_tz":360,"elapsed":205,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.scatter(train['Load [ft.lb]'], train['NOx [ppm]'],  color='blue')\n","plt.plot(train_x, theta_lin[1]*X_train_minmax + theta_lin[0], '-r')\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3qoHJ4viFsbi"},"source":["#### Now plot Predicted NOx versus Measured\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AMMwjSGcFsbj"},"source":["-   the closer to the diagonal line the better the fit\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5ZN7-1GFsbj","executionInfo":{"status":"aborted","timestamp":1647356404656,"user_tz":360,"elapsed":348,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train['NOx [ppm]'], theta_lin[1]*X_train_minmax + theta_lin[0],  color='blue', label='training')\n","plt.scatter(test['NOx [ppm]'], theta_lin[1]*X_test_minmax + theta_lin[0],  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"He3UM_hiFsbj"},"source":["#### Use sklean instead\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ptgYnHVcFsbj"},"source":["Define the model type to be a linear regression\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dg6nm6-KFsbj","executionInfo":{"status":"aborted","timestamp":1647356404656,"user_tz":360,"elapsed":348,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import linear_model\n","# model type\n","regr = linear_model.LinearRegression()\n","regr.fit (X_train_minmax, train_y)\n","# The coefficients\n","print ('Coefficients: ', regr.coef_)\n","print ('Intercept: ',regr.intercept_)\n","nc = 1\n","thetav[nc][0] = regr.intercept_\n","thetav[nc][1] = regr.coef_"]},{"cell_type":"markdown","metadata":{"id":"EP8LLHX-Fsbj"},"source":["-   the result is the same - you can plot it yourself to check\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y4dMD8PUFsbk"},"source":["### Use Gradient Descent\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zmwtNoAZFsbk"},"source":["#### Normal equation and GD with different learning rate are compared\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x9iNUI6mFsbk"},"source":["-   use python\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9mW_Jvw6Fsbk"},"source":["##### import the libraries\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wgF2RBKFsbk","executionInfo":{"status":"aborted","timestamp":1647356404657,"user_tz":360,"elapsed":349,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["#import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import math\n","import operator\n","import matplotlib.pyplot as plt\n","get_ipython().run_line_magic('matplotlib', 'inline')"]},{"cell_type":"markdown","metadata":{"id":"m_SONhBFFsbk"},"source":["##### Import the data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HQ1CYnFFsbk","executionInfo":{"status":"aborted","timestamp":1647356404657,"user_tz":360,"elapsed":155,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["data = pd.read_csv('Engine_NOx.csv')\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"MJJIOVK3Fsbk"},"source":["#### Training/test split and plot\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SuG4F2LqFsbl"},"source":["##### Training and test data is split\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNbnZdtFFsbl","executionInfo":{"status":"aborted","timestamp":1647356404657,"user_tz":360,"elapsed":155,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["cdf = data[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'NOx [ppm]']]\n","msk = np.random.rand(len(data)) < 0.8\n","train = cdf[msk]\n","test = cdf[~msk]"]},{"cell_type":"markdown","metadata":{"id":"RHZku_X_Fsbl"},"source":["##### Plot NOx versus Load\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2CbpuOCFsbl","executionInfo":{"status":"aborted","timestamp":1647356404658,"user_tz":360,"elapsed":156,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.scatter(train['Load [ft.lb]'], train['NOx [ppm]'],  color='blue', label='training')\n","plt.scatter(test['Load [ft.lb]'], test['NOx [ppm]'],  color='red', label='test')\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8aJIkue3Fsbl"},"source":["#### Now fit with Multi-features\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RAh43MgJFsbl"},"source":["Now repeat single feature regression but with multi-features\n","$ NOx =  \\theta_0 + \\text{load}\\theta_1 + \\text{speed}\\theta_2 + m_f\\theta_3 + P_r\\theta_4$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tg9QE6FUFsbl","executionInfo":{"status":"aborted","timestamp":1647356404658,"user_tz":360,"elapsed":155,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import preprocessing\n","\n","train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","train_y = np.asanyarray(train[['NOx [ppm]']])\n","\n","test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","test_y = np.asanyarray(test[['NOx [ppm]']])\n","\n","# train_x = np.asanyarray(train[['Load [ft.lb]']])\n","# train_y = np.asanyarray(train[['NOx [ppm]']])\n","\n","# test_x = np.asanyarray(test[['Load [ft.lb]']])\n","# test_y = np.asanyarray(test[['NOx [ppm]']])\n","\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)"]},{"cell_type":"markdown","metadata":{"id":"AaJhRZ3SFsbm"},"source":["#### Find theta\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6rtrZz5Fsbm","executionInfo":{"status":"aborted","timestamp":1647356404658,"user_tz":360,"elapsed":153,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["numsamples = X_train_minmax.shape[0]\n","\n","X_b = np.c_[np.ones((numsamples,1)),X_train_minmax]\n","X_t = np.c_[np.ones((X_test_minmax.shape[0],1)),X_test_minmax]\n","theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(train_y)\n","# Store theta - normal equations\n","nc = 2\n","thetav[nc:nc+1] = theta.T\n","print(theta)"]},{"cell_type":"markdown","metadata":{"id":"2pd1klrbFsbm"},"source":["#### Plot predicted versus experimental\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f65xEyxuFsbm","executionInfo":{"status":"aborted","timestamp":1647356404658,"user_tz":360,"elapsed":152,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train_y, np.dot(theta.T,X_b.T),  color='blue', label='training')\n","plt.scatter(test_y, np.dot(theta.T,X_t.T),  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"D2MtYDy7Fsbn"},"source":["#### Print fit errors (MAE, MSE, R2)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R2M5lTQGFsbn"},"source":["Import $R^2$ from sklearn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bA3NJETYFsbn","executionInfo":{"status":"aborted","timestamp":1647356404659,"user_tz":360,"elapsed":151,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.metrics import r2_score\n","mae = np.mean(np.absolute(np.dot(theta.T,X_t.T).T - test_y))\n","mse = np.mean((np.dot(theta.T,X_t.T).T - test_y) ** 2)\n","r2 = r2_score(np.dot(theta.T,X_t.T).T , test_y)\n","print(\"Multi Feature: Mean absolute error: %.2f\" % mae)\n","print(\"Multi Feature: Residual sum of squares (MSE): %.2f\" % mse)\n","print(\"Multi Feature: R2-score: %.2f\" % r2 )\n","# the last column is lambda\n","errv[nc:nc+1] = [mae, mse, r2, 0]"]},{"cell_type":"markdown","metadata":{"id":"EbaaaiAaFsbn"},"source":["#### Cost Function and Gradients\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FU8pEA6YFsbn"},"source":["-   The equation for calculating cost function and gradients are as shown below.\n","-   Please note the cost function is for Linear regression.\n","-   For other algorithms the cost function will be different and the gradients would have  to be derived from the cost functions\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hX2dmPiKFsbo"},"source":["##### Cost\n","\n"]},{"cell_type":"markdown","metadata":{"id":"S-OHX8piFsbo"},"source":["\\begin{equation}\n","J(\\theta) = 1/2m \\sum_{i=1}^{m} (h(\\theta)^{(i)} - y^{(i)})^2 \n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0RgHablIFsbo"},"source":["##### Gradient\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WhQYITvuFsbo"},"source":["\\begin{equation}\n","\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = 1/m\\sum_{i=1}^{m}(h(\\theta)^{(i)} - y^{(i)}).X_j^{(i)}\n","\\end{equation}\n","\n","Where $ h(\\theta) \\longrightarrow \\hat{y}$ the predicted output\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I8cq-afbFsbo"},"source":["##### Gradients\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5ZRbiWsrFsbo"},"source":["\\begin{equation}\n","\\theta_0: = \\theta_0 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta)^{(i)} - y^{(i)}).X_0^{(i)})\n","\\end{equation}\n","\n","\\begin{equation}\n","\\theta_1: = \\theta_1 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta)^{(i)} - y^{(i)}).X_1^{(i)})\n","\\end{equation}\n","\n","\\begin{equation}\n","\\theta_2: = \\theta_2 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta)^{(i)} - y^{(i)}).X_2^{(i)})\n","\\end{equation}\n","\n","\\begin{equation}\n","\\theta_j: = \\theta_j -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta)^{(i)} - y^{(i)}).X_0^{(i)})\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZKqi9AbzFsbo"},"source":["#### Cost Calculation function\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THdEaXvsFsbo","executionInfo":{"status":"aborted","timestamp":1647356404659,"user_tz":360,"elapsed":151,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def  cal_cost(theta,X,y):\n","    '''\n","    \n","    Calculates the cost for given X and Y. The following shows and example of a single dimensional X\n","    theta = Vector of thetas \n","    X     = Row of X's np.zeros((2,j))\n","    y     = Actual y's np.zeros((2,1))\n","    \n","    where:\n","        j is the no of features\n","    '''\n","    \n","    m = len(y)\n","    # yhat = theta^T x\n","    predictions = X.dot(theta)\n","    # sum the squares of yhat - y\n","    cost = 1/(2*m) * np.sum(np.square(predictions-y))\n","    return cost"]},{"cell_type":"markdown","metadata":{"id":"iNX3XTEmFsbp"},"source":["#### Gradient Descent function\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSn8RjlhFsbp","executionInfo":{"status":"aborted","timestamp":1647356404660,"user_tz":360,"elapsed":9,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def gradient_descent(X,y,theta,learning_rate=0.01,iterations=100):\n","    '''\n","    X    = Matrix of X with added bias units\n","    y    = Vector of Y\n","    theta=Vector of thetas np.random.randn(j,1)\n","    learning_rate \n","    iterations = no of iterations\n","    \n","    Returns the final theta vector and array of cost history over no of iterations\n","    '''\n","    m = len(y)\n","    cost_history = np.zeros(iterations)\n","    theta_history = np.zeros((iterations,5))\n","    for it in range(iterations):\n","        \n","        prediction = np.dot(X, theta)\n","        \n","        theta = theta -(1/m)*learning_rate*( X.T.dot((prediction - y)))\n","        theta_history[it,:] =theta.T\n","        cost_history[it]  = cal_cost(theta,X,y)\n","        \n","    return theta, cost_history, theta_history"]},{"cell_type":"markdown","metadata":{"id":"KtowS92bFsbp"},"source":["#### Gradient Descent for NOx modeling\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yef04dKFsbp","executionInfo":{"status":"aborted","timestamp":1647356404660,"user_tz":360,"elapsed":9,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["lr =0.1\n","n_iter = 1000\n","\n","theta = np.random.randn(5,1)\n","\n","\n","X_b = np.c_[np.ones((len(X_train_minmax),1)),X_train_minmax]\n","\n","theta,cost_history,theta_history = gradient_descent(X_b,train_y,theta,lr,n_iter)"]},{"cell_type":"markdown","metadata":{"id":"ESU3-UNHFsbp"},"source":["-   print result\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqocDdZpFsbp","executionInfo":{"status":"aborted","timestamp":1647356404660,"user_tz":360,"elapsed":8,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["print('Theta0: {:0.3f},\\nTheta1: {:0.3f},\\nTheta2: {:0.3f},\\nTheta3: {:0.3f},\\nTheta4: {:0.3f}'.format(theta[0][0],theta[1][0],theta[2][0],theta[3][0],theta[4][0]))\n","print('Final cost:  {:0.3f}'.format(cost_history[-1]))\n","# Store theta - gradient\n","nc = 3\n","thetav[nc:nc+1] = theta.T\n","print(theta)"]},{"cell_type":"markdown","metadata":{"id":"Wzm1mna6Fsbp"},"source":["#### Plot - the cost J as a function of iterations\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Tlp6qMxvFsbp"},"source":["-   set figure axis and labels\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yN39X_1SFsbq","executionInfo":{"status":"aborted","timestamp":1647356404661,"user_tz":360,"elapsed":9,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["fig,ax = plt.subplots(figsize=(12,8))\n","\n","ax.set_ylabel('J(Theta)')\n","ax.set_xlabel('Iterations')\n","_=ax.plot(range(n_iter),cost_history,'b.')\n","ax.set_yscale('log')"]},{"cell_type":"markdown","metadata":{"id":"ovQHdqaqFsbq"},"source":["##### Plot Predicted NOx versus Experimental\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HzWFCqNiFsbq","executionInfo":{"status":"aborted","timestamp":1647356404661,"user_tz":360,"elapsed":8,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train_y, np.dot(theta.T,X_b.T),  color='blue', label='training')\n","plt.scatter(test_y, np.dot(theta.T,X_t.T),  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9OrZXVvNFsbq"},"source":["##### Print MAE, MSE and R2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6H4Mf-LwFsbq","executionInfo":{"status":"aborted","timestamp":1647356404661,"user_tz":360,"elapsed":7,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.metrics import r2_score\n","mae = np.mean(np.absolute(np.dot(theta.T,X_t.T).T - test_y))\n","mse = np.mean((np.dot(theta.T,X_t.T).T - test_y) ** 2)\n","r2 = r2_score(np.dot(theta.T,X_t.T).T , test_y)\n","print(\"Multi Feature: Mean absolute error: %.2f\" % mae)\n","print(\"Multi Feature: Residual sum of squares (MSE): %.2f\" % mse)\n","print(\"Multi Feature: R2-score: %.2f\" % r2 )\n","# the last column is lambda\n","errv[nc:nc+1] = [mae, mse, r2, 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WEkrfjNFsbq","executionInfo":{"status":"aborted","timestamp":1647356404662,"user_tz":360,"elapsed":2033,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["thetav"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSfN7Y9RFsbq","executionInfo":{"status":"aborted","timestamp":1647356404662,"user_tz":360,"elapsed":2029,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["#dftheta = pd.DataFrame(thetav)\n","# dftheta = pd.DataFrame.columns = ['Theta0','Theta1','Theta2','Theta3','Theta4']\n","#dftheta\n","# thetav\n","df = pd.DataFrame(np.random.randn(10, 4))\n","df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\"))"]},{"cell_type":"markdown","metadata":{"id":"fjfgcLl6Fsbr"},"source":["### Regularized Objective\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hLO9XB9PFsbr"},"source":["#### Least Squares (LS) underdetermined - add regularized objective term\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8BFwDS71Fsbr"},"source":["-   for the exact computation of $\\hat{ \\mathbf{\\theta}}_{LS}$ considering the problem geometrically provides intuition\n","-   The invertability of $X^T X$ implies the unique existence of a set of parameters $ \\mathbf{\\theta}$ that minimizes \\eqref{eq:least_squares}\n","-   In the case that we have fewer linearly independent data points than $n$, \\eqref{eq:LS_grad2} is not unique.\n","    -   Then there exist multiple values of $ \\mathbf{\\theta}$ that result in zero error\n","    -   and the system is underdetermined.\n","-   A common approach to this is to optimize a regularized objective\n","\n","\\begin{equation}\n","\\begin{aligned}\n","& \\underset{ \\mathbf{\\theta}}{\\min}\n","& & \\left\\|  \\mathbf{y} - X  \\mathbf{\\theta} \\right\\|_2^2 + \\lambda \\| \\mathbf{\\theta}\\|^2_2\n","\\end{aligned}\n","\\label{eq:least_squares_norm}\n","\\end{equation}\n","\n","where $\\lambda$ is a positive scalar.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dTDPqBMRFsbr"},"source":["#### Ridge or Tikhanov regression\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gsplCI2rFsbr"},"source":["-   By adding in the regularization term $\\| \\mathbf{\\theta}\\|^2_2$\n","    -   typically referred to as Ridge or Tikhanov regression, or $L_2$ regression due to the use of the 2-norm)\n","    -   the optimal value of $ \\mathbf{\\theta}$ is biased toward having a small norm.\n","    -   Following a similar approach that resulted in of \\eqref{eq:LS_grad2}, gives\n","\n","\\begin{equation}\n","    \\hat{ \\mathbf{\\theta}} = (X^T X + \\lambda I)^{-1} X^T  \\mathbf{y}\n","    \\label{eq:LS_grad}\n","\\end{equation}\n","\n","-   for which the $X^T X + \\lambda I$ is always full rank and thus invertible.\n","-   the solution to the ridge regularized problem is denoted $\\hat{ \\mathbf{\\theta}}_{RR}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hzC_qpZOFsbr"},"source":["#### Ridge regression continued\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HzeZ5KTfFsbr"},"source":["-   for positive $\\lambda$, $\\|  \\mathbf{y} - X  \\mathbf{\\theta}\\|_2^2$ may not be zero\n","    -   the estimator of the parameters is \\textit{biased}.\n","    -   As $\\lambda$ gets smaller, this bias will decrease.\n","-   In the limit of $\\lambda \\to 0$, we have\n","\n","\\begin{equation}\n","    (X^T X + \\lambda I)^{-1} X^T \\to X^T (X X^T)^{-1}\n","\\end{equation}\n","\n","-   the \\textit{least norm} solution to \\eqref{eq:LS_grad2} is\n","\n","\\begin{equation}\n","    \\hat{ \\mathbf{\\theta}}_{LN} = X^T (X X^T)^{-1}  \\mathbf{y}.\n","    \\label{eq:least_norm}\n","\\end{equation}\n","\n","-   the least norm solution is the set of parameters that achieves zero loss on \\eqref{eq:least_squares}\n","-   while simultaneously minimizing the 2-norm of the parameter vector.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Sb6pV3WwFsbr"},"source":["#### Least norm vs ridge regression\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a19vXUO6Fsbs"},"source":["-   why use the ridge regression with a positive value of $\\lambda$\n","-   The added bias from the regularization helps generalize to new data points without <span class=\"underline\">overfitting</span> to the training dataset.\n","-   Regularization is an important concept in machine learning discussed later\n","-   further, ridge regression problem arises naturally in Bayesian inference\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PjY31U22Fsbs"},"source":["### Ridge regression example in Python\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lnLLM1yJFsbs"},"source":["#### Ridge regression single feature: Eq. \\ref{eq:LS_grad}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eTCzplueFsbs"},"source":["Now repeat single feature regression but with ridge regularization\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6XeLGEFFsbs","executionInfo":{"status":"aborted","timestamp":1647356404662,"user_tz":360,"elapsed":2028,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import preprocessing\n","\n","train_x = np.asanyarray(train[['Load [ft.lb]']])\n","train_y = np.asanyarray(train[['NOx [ppm]']])\n","\n","test_x = np.asanyarray(test[['Load [ft.lb]']])\n","test_y = np.asanyarray(test[['NOx [ppm]']])\n","\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPKd7QOvFsbs","executionInfo":{"status":"aborted","timestamp":1647356404663,"user_tz":360,"elapsed":2028,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["numsamples = X_train_minmax.shape[0]\n","# 1st row is ones and second row is x\n","Xtrain = np.c_[np.ones((numsamples,1)),X_train_minmax]\n","ytrain = train_y"]},{"cell_type":"markdown","metadata":{"id":"K64OdDoXFsbs"},"source":["Use regularization from Eq. \\ref{eq:least_squares_norm}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGOp5I6tFsbs","executionInfo":{"status":"aborted","timestamp":1647356404663,"user_tz":360,"elapsed":2026,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["Lambda = 0.1\n","\n","size = Xtrain.shape\n","theta_ridge = np.dot(np.dot(np.linalg.inv(\n","            np.add(np.dot(Xtrain.T, Xtrain) / 1, np.multiply(Lambda, np.identity(size[1])))),\n","                                     Xtrain.T), ytrain) / 1\n","\n","# The coefficients\n","print ('Coefficients: ', theta_ridge[1])\n","print ('Intercept: ',theta_ridge[0])"]},{"cell_type":"markdown","metadata":{"id":"2l-hE7YCFsbt"},"source":["#### Plot fit on training data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abW66eoiFsbt","executionInfo":{"status":"aborted","timestamp":1647356404663,"user_tz":360,"elapsed":2025,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.scatter(train['Load [ft.lb]'], train['NOx [ppm]'],  color='blue')\n","plt.plot(train_x, theta_ridge[0]+X_train_minmax * theta_ridge[1], '-r')\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ot4ozhGQFsbt"},"source":["##### Plot NOx prediction versus Experimental\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ik2MNPAXFsbt","executionInfo":{"status":"aborted","timestamp":1647356404663,"user_tz":360,"elapsed":2024,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train['NOx [ppm]'], theta_ridge[0]*X_train_minmax + theta_lin[1],  color='blue', label='training')\n","plt.scatter(test['NOx [ppm]'], theta_ridge[0]*X_test_minmax + theta_lin[1],  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9D9JHec2Fsbt"},"source":["#### Ridge regression example 1 - linear with one feature\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_QA9nzLGFsbt"},"source":["-   in Python now use `sklearn` library\n","-   Import ridge `Ridge` from `sklearn`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3B2-JNYTFsbt","executionInfo":{"status":"aborted","timestamp":1647356404664,"user_tz":360,"elapsed":2024,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","#  definition of cost is differert in sklearn - gradient decsent is used here\n","Lambda = 0.1\n","# create regressor\n","regr_ridge = Ridge(alpha=Lambda)\n","# train regressor\n","regr_ridge.fit(X_train_minmax, train_y)\n","\n","# The coefficients\n","print ('Coefficients: ', regr_ridge.coef_)\n","print ('Intercept: ',regr_ridge.intercept_)"]},{"cell_type":"markdown","metadata":{"id":"qmh46uc3Fsbt"},"source":["#### Ex 1: Now plot linear with one feature\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IeQIt_AZFsbu","executionInfo":{"status":"aborted","timestamp":1647356404664,"user_tz":360,"elapsed":2023,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.scatter(train['Load [ft.lb]'], train['NOx [ppm]'],  color='blue')\n","plt.plot(train_x, regr_ridge.coef_[0][0]*X_train_minmax + regr_ridge.intercept_[0], '-r')\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47FJXtrxFsbu","executionInfo":{"status":"aborted","timestamp":1647356404664,"user_tz":360,"elapsed":2022,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train['NOx [ppm]'], regr_ridge.coef_[0][0]*X_train_minmax + regr_ridge.intercept_[0],  color='blue', label='training')\n","plt.scatter(test['NOx [ppm]'], regr_ridge.coef_[0][0]*X_test_minmax + regr_ridge.intercept_[0],  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bjYPR-vLFsbu"},"source":["#### Ridge regression example 2 - linear with multiple feature\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Lu8tAs4gFsbu"},"source":["-   here try to improve the NOx model using:\n","    -   more features as before use: load, speed, $m_f$, $P_r$ and a constant\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogmW1ZVPFsbu","executionInfo":{"status":"aborted","timestamp":1647356404665,"user_tz":360,"elapsed":2022,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["#  Preparing data for sklearn\n","from sklearn import preprocessing\n","\n","train_x_miso = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","train_y_miso = np.asanyarray(train[['NOx [ppm]']])\n","\n","test_x_miso = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","test_y_miso = np.asanyarray(test[['NOx [ppm]']])\n","\n","\n","min_max_scaler_miso = preprocessing.MinMaxScaler()\n","X_train_minmax_miso = min_max_scaler.fit_transform(train_x_miso)\n","X_test_minmax_miso = min_max_scaler.transform(test_x_miso)"]},{"cell_type":"markdown","metadata":{"id":"UobOomEJFsbu"},"source":["#### EX 2: Fit the model and print coefficients\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LpToo_tlFsbv","executionInfo":{"status":"aborted","timestamp":1647356404665,"user_tz":360,"elapsed":2021,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import linear_model\n","regr_miso = linear_model.LinearRegression()\n","regr_miso.fit (X_train_minmax_miso, train_y)\n","# The coefficients\n","print ('Coefficients: ', regr_miso.coef_)\n","print ('Intercept: ',regr_miso.intercept_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uILB-lo7Fsbv","executionInfo":{"status":"aborted","timestamp":1647356404665,"user_tz":360,"elapsed":2019,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["y_hat_regr_miso_tr = regr_miso.predict(X_train_minmax_miso) #training prediction\n","y_hat_regr_miso_ts = regr_miso.predict(X_test_minmax_miso) #testing prediction"]},{"cell_type":"markdown","metadata":{"id":"eF7NKI8zFsbv"},"source":["##### Plot NOx fit\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuHgwrstFsbv","executionInfo":{"status":"aborted","timestamp":1647356404666,"user_tz":360,"elapsed":2019,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train['NOx [ppm]'], y_hat_regr_miso_tr,  color='blue', label='training')\n","plt.scatter(test['NOx [ppm]'], y_hat_regr_miso_ts,  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xnoWU5bGFsbv"},"source":["#### Compare Ex 1 (single) and Ex 2 Multi feature model use - MSE, MAE, and \\(R^2\\)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWP-ptGmFsbv","executionInfo":{"status":"aborted","timestamp":1647356404666,"user_tz":360,"elapsed":2018,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.metrics import r2_score\n","\n","\n","print(\"Multi Feature: Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_miso_ts - test_y)))\n","print(\"Multi Feature: Residual sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_miso_ts - test_y) ** 2))\n","print(\"Multi Feature: R2-score: %.2f\" % r2_score(y_hat_regr_miso_ts , test_y) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNtZAPDjFsby","executionInfo":{"status":"aborted","timestamp":1647356404666,"user_tz":360,"elapsed":2017,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["y_hat_regr_tr = regr.predict(X_train_minmax) #training prediction\n","y_hat_regr_ts = regr.predict(X_test_minmax) #testing prediction\n","\n","print(\"Single Feature: Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_ts - test_y)))\n","print(\"Single Feature: Residual sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_ts - test_y) ** 2))\n","print(\"Single Feature: R2-score: %.2f\" % r2_score(y_hat_regr_ts , test_y) )"]},{"cell_type":"markdown","metadata":{"id":"FsEqi-78Fsby"},"source":["#### Example 3 add regularization to example 2 (multi-feature)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDXB09CvFsby","executionInfo":{"status":"aborted","timestamp":1647356404666,"user_tz":360,"elapsed":2016,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["regr_ridge_miso = Ridge(alpha=0.1)\n","regr_ridge_miso.fit(X_train_minmax_miso, train_y)\n","\n","# The coefficients\n","print ('Coefficients: ', regr_ridge_miso.coef_)\n","print ('Intercept: ',regr_ridge_miso.intercept_)"]},{"cell_type":"markdown","metadata":{"id":"fdfz9-NvFsby"},"source":["#### Ex 3: multiple features prediction and plot\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nmrJX0kFsbz","executionInfo":{"status":"aborted","timestamp":1647356404667,"user_tz":360,"elapsed":2016,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["y_hat_regr_ridge_miso_tr = regr_ridge_miso.predict(X_train_minmax_miso) #training prediction\n","y_hat_regr_ridge_miso_ts = regr_ridge_miso.predict(X_test_minmax_miso) #testing prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tbs9daibFsbz","executionInfo":{"status":"aborted","timestamp":1647356404667,"user_tz":360,"elapsed":2015,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train['NOx [ppm]'], y_hat_regr_ridge_miso_tr,  color='blue', label='training')\n","plt.scatter(test['NOx [ppm]'], y_hat_regr_ridge_miso_ts,  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()\n","\n","print(\"Multi Feature (Ridge): Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_ridge_miso_ts - test_y)))\n","print(\"Multi Feature (Ridge): Residual sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_ridge_miso_ts - test_y) ** 2))\n","print(\"Multi Feature (Ridge): R2-score: %.2f\" % r2_score(y_hat_regr_ridge_miso_ts , test_y) )"]},{"cell_type":"markdown","metadata":{"id":"s6gtOZ2XFsbz"},"source":["-   Mean absolute error and Residual sum of squares (MSE) increased slightly over the test data while R2 is the same\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ktaYiI7vFsbz"},"source":["#### Example 4 Use polynomial features\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Svg6cpUBFsbz"},"source":["-   first import function to create polynomial features\n","-   get 15 terms with all cross products\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZzDrRKBFsb0","executionInfo":{"status":"aborted","timestamp":1647356404842,"user_tz":360,"elapsed":2189,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","train_x_p = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","train_y_p = np.asanyarray(train[['NOx [ppm]']])\n","\n","test_x_p = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","test_y_p = np.asanyarray(test[['NOx [ppm]']])\n","# here use polynomials of degree 2\n","poly = PolynomialFeatures(degree=2)\n","train_x_poly = poly.fit_transform(train_x_p)\n","test_x_poly = poly.fit_transform(test_x_p)\n","\n","min_max_scaler_ploy = preprocessing.MinMaxScaler()\n","X_train_minmax_ploy = min_max_scaler.fit_transform(train_x_poly)\n","X_test_minmax_ploy = min_max_scaler.transform(test_x_poly)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNJMN76GFsb0","executionInfo":{"status":"aborted","timestamp":1647356404842,"user_tz":360,"elapsed":2188,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["regr_poly = linear_model.LinearRegression()\n","\n","regr_poly.fit (X_train_minmax_ploy, train_y_p)\n","# The coefficients\n","print ('Coefficients: ', regr_poly.coef_)\n","print ('Intercept: ',regr_poly.intercept_)"]},{"cell_type":"markdown","metadata":{"id":"TpBsCk4eFsb0"},"source":["#### Ex 4: training and test prediction and plot for polynomial features\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QdFDPfGlFsb0"},"source":["-   now, perform multi-feature regression with polynomial basis\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_j9pGX9kFsb0","executionInfo":{"status":"aborted","timestamp":1647356404842,"user_tz":360,"elapsed":2187,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["y_hat_regr_poly_tr = regr_poly.predict(X_train_minmax_ploy) #training prediction\n","y_hat_regr_poly_ts = regr_poly.predict(X_test_minmax_ploy) #testing prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"97GXkKy_Fsb0","executionInfo":{"status":"aborted","timestamp":1647356404843,"user_tz":360,"elapsed":2186,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train['NOx [ppm]'], y_hat_regr_poly_tr,  color='blue', label='training')\n","plt.scatter(test['NOx [ppm]'], y_hat_regr_poly_ts,  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9NbOxA4Fsb1","executionInfo":{"status":"aborted","timestamp":1647356404843,"user_tz":360,"elapsed":2185,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["print(\"Multi Feature + polynomial- testing Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_poly_ts - test_y)))\n","print(\"Multi Feature + polynomial- testing sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_poly_ts - test_y) ** 2))\n","print(\"Multi Feature + polynomial- testing R2-score: %.2f\" % r2_score(y_hat_regr_poly_ts , test_y) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5VAV5flFsb1","executionInfo":{"status":"aborted","timestamp":1647356404843,"user_tz":360,"elapsed":2184,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["# training: \n","print(\"Multi Feature + polynomial- training Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_poly_tr - train_y)))\n","print(\"Multi Feature + polynomial- training sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_poly_tr - train_y) ** 2))\n","print(\"Multi Feature + polynomial- training R2-score: %.2f\" % r2_score(y_hat_regr_poly_tr , train_y) )"]},{"cell_type":"markdown","metadata":{"id":"tMpysl8lFsb1"},"source":["#### Ex 5: try regularization with the polynomial feature set\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXi5DsyjFsb1","executionInfo":{"status":"aborted","timestamp":1647356404844,"user_tz":360,"elapsed":2184,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["#  definition of cost is differert in sklearn - gradient decsent is used here\n","\n","regr_ridge_poly = Ridge(alpha=0.001)\n","regr_ridge_poly.fit(X_train_minmax_ploy, train_y)\n","\n","# The coefficients\n","print ('Coefficients: ', regr_ridge_poly.coef_)\n","print ('Intercept: ',regr_ridge_poly.intercept_)"]},{"cell_type":"markdown","metadata":{"id":"wYW1GUkaFsb1"},"source":["#### Ex 5: Plot poly features with regularization\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVtqITlDFsb1","executionInfo":{"status":"aborted","timestamp":1647356404844,"user_tz":360,"elapsed":2183,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["y_hat_regr_ridge_poly_tr = regr_ridge_poly.predict(X_train_minmax_ploy) #training prediction\n","y_hat_regr_ridge_poly_ts = regr_ridge_poly.predict(X_test_minmax_ploy) #testing prediction\n","\n","\n","plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train['NOx [ppm]'], y_hat_regr_ridge_poly_tr,  color='blue', label='training')\n","plt.scatter(test['NOx [ppm]'], y_hat_regr_ridge_poly_ts,  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()\n","\n","\n","print(\"Multi Feature + polynomial (Ridge)-testing Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_ridge_poly_ts - test_y)))\n","print(\"Multi Feature + polynomial (Ridge)-testing Residual sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_ridge_poly_ts - test_y) ** 2))\n","print(\"Multi Feature + polynomial (Ridge)-testing R2-score: %.2f\" % r2_score(y_hat_regr_ridge_poly_ts , test_y) )"]},{"cell_type":"markdown","metadata":{"id":"dpapo_-dFsb1"},"source":["#### EX 5: Mean absolute error and Residual sum of squares (MSE) is slightly increased for testing data, while \\(R^2\\) is almost the same.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c87kn2DdFsb1","executionInfo":{"status":"aborted","timestamp":1647356404844,"user_tz":360,"elapsed":2182,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["# training: \n","print(\"Multi Feature + polynomial + regularization- training Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_ridge_poly_tr - train_y)))\n","print(\"Multi Feature + polynomial + regularization- training sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_ridge_poly_tr - train_y) ** 2))\n","print(\"Multi Feature + polynomial +regularization- training R2-score: %.2f\" % r2_score(y_hat_regr_ridge_poly_tr , train_y) )"]},{"cell_type":"markdown","metadata":{"id":"aK-2O_XmFsb2"},"source":["#### For Ex 5: analyze the effect of  \\( \\lambda \\) in MAE:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqX5HhfVFsb2","executionInfo":{"status":"aborted","timestamp":1647356404844,"user_tz":360,"elapsed":2181,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","from sklearn.metrics import mean_squared_error\n","\n","clf = Ridge()\n","\n","X = X_train_minmax_ploy\n","y = train_y\n","\n","coefs = []\n","errors = []\n","\n","alphas = np.logspace(-6, 6, 200)\n","\n","# Train the model with different regularisation strengths\n","for a in alphas:\n","    clf.set_params(alpha=a)\n","    clf.fit(X, y)\n","    coefs.append(clf.coef_)\n","    errors.append(np.mean(np.absolute(clf.predict(X_test_minmax_ploy) - test_y)))"]},{"cell_type":"markdown","metadata":{"id":"Yfo5GrBhFsb2"},"source":["#### EX 5: Plot MAE versus lambda to see the effect of regularization\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJIRrSA6Fsb2","executionInfo":{"status":"aborted","timestamp":1647356404845,"user_tz":360,"elapsed":2181,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.figure(figsize=(20, 6))\n","\n","\n","ax = plt.gca()\n","ax.plot(alphas, errors)\n","ax.set_xscale('log')\n","plt.xlabel('$\\lambda$')\n","plt.ylabel('MAE (e)')\n","plt.title('Coefficient error as a function of the regularization')\n","plt.axis('tight')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"r0xEqG6wFsb2"},"source":["-   setting 10e-3 for `lambda` decreases the error\n","-   but increasing more may give us a model with smaller weights in loss of accuracy\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A7HFLGVLFsb2"},"source":["### Lasso with Python example\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xObttj3LFsb2"},"source":["#### Lasso regularization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rXWh4dN5Fsb2"},"source":["-   Lasso: Least absolute shrinkage and selection operator can be written as:\n","\n","\\begin{equation}\n","\\begin{aligned}\n","& \\underset{ \\mathbf{\\theta}}{\\min}\n","& & \\left\\|  \\mathbf{y} - X  \\mathbf{\\theta} \\right\\|_2^2 + \\lambda \\| \\mathbf{\\theta}\\|_2\n","\\end{aligned}\n","\\label{eq:lasso_norm}\n","\\end{equation}\n","\n","-   where $\\lambda$ is a positive scalar\n","-   here the magnitude of $ \\mathbf{\\theta}$ is used not the square as in the ridge regression\n","-   can help with overfitting and also with feature selection\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P96cHdqqFsb2"},"source":["#### Lasso regression\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MkDq8tvvFsb3"},"source":["-   LASSO is a regression method that also perform feature selection and regularization to improve the model's prediction accuracy.\n","-   plan: choose the most significant feature from\n","    -   multi-feature and\n","    -   multi-feature with polynomial features\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w9K02OJDFsb3"},"source":["#### For Python Example\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BvuFKTLBFsb3"},"source":["-   import and fit to training data\n","-   note $\\lambda$ above is defined as `alpha` in `sklearn` for lasso\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pe67mgSFsb3","executionInfo":{"status":"aborted","timestamp":1647356404845,"user_tz":360,"elapsed":2180,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import linear_model\n","regr_Lasso_miso = linear_model.Lasso(alpha=0.2)\n","regr_Lasso_miso.fit(X_train_minmax_miso, train_y)\n","# The coefficients\n","print ('Coefficients: ', regr_Lasso_miso.coef_)\n","print ('Intercept: ',regr_Lasso_miso.intercept_)"]},{"cell_type":"markdown","metadata":{"id":"AAZ01FzBFsb3"},"source":["#### Ex 1: Lasso prediction for training data - linear model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMjbHENAFsb3","executionInfo":{"status":"aborted","timestamp":1647356404845,"user_tz":360,"elapsed":2178,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["y_hat_regr_Lasso_miso_tr = regr_Lasso_miso.predict(X_train_minmax_miso) #training prediction\n","y_hat_regr_Lasso_miso_ts = regr_Lasso_miso.predict(X_test_minmax_miso) #testing prediction\n","\n","plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train['NOx [ppm]'], y_hat_regr_Lasso_miso_tr,  color='blue', label='training')\n","plt.scatter(test['NOx [ppm]'], y_hat_regr_Lasso_miso_ts,  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()\n","\n","print(\"Multi Feature + polynomial (LASSO)- testing Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_Lasso_miso_ts - test_y)))\n","print(\"Multi Feature + polynomial (LASSO)- testing Residual sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_Lasso_miso_ts - test_y) ** 2))\n","print(\"Multi Feature + polynomial (LASSO)- testing R2-score: %.2f\" % r2_score(y_hat_regr_Lasso_miso_ts , test_y) )"]},{"cell_type":"markdown","metadata":{"id":"Y3eFINHbFsb4"},"source":["#### Ex 1: Lasso prediction for test data - linear model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"952CqOQjFsb4","executionInfo":{"status":"aborted","timestamp":1647356404846,"user_tz":360,"elapsed":2178,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["print(\"Multi Feature + polynomial (LASSO)- training Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_Lasso_miso_tr - train_y)))\n","print(\"Multi Feature + polynomial (LASSO)- training sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_Lasso_miso_tr - train_y) ** 2))\n","print(\"Multi Feature + polynomial (LASSO)- training R2-score: %.2f\" % r2_score(y_hat_regr_Lasso_miso_tr , train_y) )"]},{"cell_type":"markdown","metadata":{"id":"oCkNDIivFsb4"},"source":["-   loss of accuracy, but now only one feature instead of four features\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8uLGMp92Fsb4"},"source":["#### Ex 2: Lasso prediction for training/test data - polynomial feature set\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8B6ovUmFsb4","executionInfo":{"status":"aborted","timestamp":1647356404846,"user_tz":360,"elapsed":2177,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["regr_Lasso_poly = linear_model.Lasso(alpha=0.1, max_iter=5000)\n","\n","regr_Lasso_poly.fit(X_train_minmax_ploy, train_y)\n","# The coefficients\n","print ('Coefficients: ', regr_Lasso_poly.coef_)\n","print ('Intercept: ',regr_Lasso_poly.intercept_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVtuL3qMFsb4","executionInfo":{"status":"aborted","timestamp":1647356404846,"user_tz":360,"elapsed":2176,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["y_hat_regr_Lasso_poly_tr = regr_Lasso_poly.predict(X_train_minmax_ploy) #training prediction\n","y_hat_regr_Lasso_poly_ts = regr_Lasso_poly.predict(X_test_minmax_ploy) #testing prediction\n","\n","\n","plt.plot([100,350], [100, 350],  '-k')\n","plt.scatter(train['NOx [ppm]'], y_hat_regr_Lasso_poly_tr,  color='blue', label='training')\n","plt.scatter(test['NOx [ppm]'], y_hat_regr_Lasso_poly_ts,  color='red', label='testing')\n","plt.xlabel(\"Exp NOx [ppm]\")\n","plt.ylabel(\"Pred NOx [ppm]\")\n","plt.legend()\n","plt.gca().set_aspect('equal', adjustable='box')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"MTXj3fMpFsb4"},"source":["#### Compare Ex1 and Ex 2 Lasso prediction for training/test data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtbTLx2PFsb5","executionInfo":{"status":"aborted","timestamp":1647356404846,"user_tz":360,"elapsed":2175,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["# test\n","print(\"Multi Feature + polynomial (LASSO)- testing Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_Lasso_poly_ts - test_y)))\n","print(\"Multi Feature + polynomial (LASSO)- testing Residual sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_Lasso_poly_ts - test_y) ** 2))\n","print(\"Multi Feature + polynomial (LASSO)- testing R2-score: %.2f\" % r2_score(y_hat_regr_Lasso_poly_ts , test_y) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14CjEcWdFsb5","executionInfo":{"status":"aborted","timestamp":1647356404847,"user_tz":360,"elapsed":2174,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["# training: \n","print(\"Multi Feature + polynomial (LASSO)- training Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat_regr_Lasso_poly_tr - train_y)))\n","print(\"Multi Feature + polynomial (LASSO)- training sum of squares (MSE): %.2f\" % np.mean((y_hat_regr_Lasso_poly_tr - train_y) ** 2))\n","print(\"Multi Feature + polynomial (LASSO)- training R2-score: %.2f\" % r2_score(y_hat_regr_Lasso_poly_tr , train_y) )"]},{"cell_type":"markdown","metadata":{"id":"2At18u7-Fsb5"},"source":["-   for the polynomial feature set compared to the linear fit\n","    -   four features were removed without significantly losing any accuracy\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VeF2RKHVFsb5"},"source":["### Logistic Regression\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m-NQU1V7Fsb5"},"source":["#### Logistic Regression is commonly used to\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MCZDM0whFsb5"},"source":["-   estimate the probability that an instance belongs to a particular class\n","-   (e.g., what is the probability that this email is spam?).\n","-   If the estimated probability is greater than 50%,\n","    -   then the model predicts that the instance belongs to that class\n","    -   (called the positive class, labeled \\`\\`1''),\n","    -   and otherwise it predicts that it does not\n","    -   (i.e., it belongs to the negative class, labeled \\`\\`0'').\n","-   This makes it a binary classifier.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G4Q0Wb-KFsb5"},"source":["#### Details of Logistic Regression\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LNvtEDyLFsb5"},"source":["-   Just like a Linear Regression model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term),\n","-   but instead of outputting the result directly like the Linear Regression model does,\n","-   it outputs the logistic of this result\n","\n","\\begin{equation}\n","    \\hat{y} = \\sigma (w^T x + b)\n","\\end{equation}\n","\n","-   where $ \\sigma$ is sigmoid function defined as\n","\n","\\begin{equation}\\label{eq:sig}\n","\\begin{split}\n","\\sigma(z) & = \\frac{1}{1 + e^{-z}} \\\\\n","\\frac{d}{dz} \\sigma(z) & = \\sigma(z) (1 - \\sigma(z))\n","\\end{split}\n","\\end{equation}\n","\n","-   Notice that $\\sigma(z)<0.5$ when $z < 0$, and $\\sigma(z) \\leq 0.5$ when $z \\leq 0$,\n","    -   so a Logistic Regression model predicts 1 if $x^T \\theta$ is positive\n","    -   and 0 if it is negative.\n","-   for given data set of $ {(x^{(1)},y^{(1)}), ..., (x^{(m)},y^{(m)})} $,\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gU4rN1WLFsb5"},"source":["#### Details of Logistic Regression - cost function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vOuNaL_IFsb6"},"source":["-   the cost function of Logistic Regression can be defined as\n","\n","\\begin{equation}\n","    J = L(\\hat{y}, y) = -\\frac{1}{m}\\sum_{i=1}^{m} (y^{(i)}\\log(\\hat{y}^{(i)})+(1-y^{(i)})\\log(1-\\hat{y}^{(i)}))\n","\\end{equation}\n","\n","-   where $ i$ represent training data index.\n","-   This cost function makes sense because $ -\\log(z) $ grows very large when z approaches 0,\n","-   so the cost will be large if the model estimates a probability close to 0 for a positive instance,\n","-   and it will also be very large if the model estimates a probability close to 1 for a negative instance.\n","-   On the other hand, $ -\\log(z) $ is close to 0 when $ z $ is close to 1,\n","-   so the cost will be close to 0 if the estimated probability is close to 0 for a negative instance\n","-   or close to 1 for a positive instance,\n","-   which is precisely what we want.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZPHTkmRWFsb6"},"source":["#### Equation derivation for a single data set\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rfevKyCCFsb6"},"source":["-   derive equation first for single data set\n","-   later we need to run these equation if `for` loop for all equations.\n","-   In this case, $m = 1$ and cost function equals to loss function.\n","\n","\\begin{equation}\n","J(w, b) = L(\\hat{Y}, y)\n","\\end{equation}\n","\n","-   and as $\\hat{y} = a$, loss function is\n","\n","\\begin{equation}\n","J(w, b) = L(a, y)\n","\\end{equation}\n","\n","-   To find $dw$ we need to calculated derivate of cost function with respect to $w$:\n","\n","\\begin{equation}\n","dw = \\frac{\\partial L(a, y)}{ \\partial w}\n","\\end{equation}\n","\n","-   Based of rule of chain this derivative can be rewritten as\n","\n","\\begin{equation}\n","dw = \\frac{\\partial L(a, y)}{ \\partial a} \\frac{\\partial a}{ \\partial z} \\frac{\\partial z}{ \\partial w}\n","\\end{equation}\n","\n","-   define $da$ and $dz$ as\n","\n","\\begin{equation}\n","da = \\frac{\\partial L(a, y)}{ \\partial a}\n","\\end{equation}\n","\n","\\begin{equation}\\label{dz}\n","dz = \\frac{\\partial L(a, y)}{ \\partial a} \\frac{\\partial a}{ \\partial z}\n","\\end{equation}\n","\n","-   now first calculate $da$ using the logistic loss function\n","\n","\\begin{equation} \\label{da}\n","\\begin{split}\n","da &= \\frac{\\partial}{ \\partial a} (-y \\log(a) - (1-y) \\log(1-a))\\\\\n","& = \\frac{-y}{a} + \\frac{1-y}{1-a}\n","\\end{split}\n","\\end{equation}\n","\n","Then using Eq. \\ref{dz} and using sigmoid function, $a = \\sigma(z)$\n","$dz$ can be calculated as\n","\n","\\begin{equation}\\label{dz2}\n","\\begin{split}\n","dz &= \\frac{\\partial L(a, y)}{ \\partial a} \\frac{\\partial a}{ \\partial z}\\\\\n","&= da \\frac{\\partial a}{ \\partial z}\\\\\n","&=da (a)(1-a)\n","\\end{split}\n","\\end{equation}\n","\n","-   Substituting Eq. \\ref{da} in Eq. \\ref{dz2}, $da$ can be simplified as\n","\n","\\begin{equation}\\label{dz3}\n","\\begin{split}\n","dz&=\\frac{-y}{a} + \\frac{1-y}{1-a} (a)(1-a) \\\\\n","&= a - y\n","\\end{split}\n","\\end{equation}\n","\n","-   finally, substitute $\\partial z / \\partial w = x$, and $dw$ can be found as\n","\n","\\begin{equation}\\label{dw}\n","\\begin{split}\n","dz&= a - y\\\\\n","dw &= dz.x\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Jdug3nXiFsb6"},"source":["#### one iteration of gradient descent\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wGY3kG1uFsb6"},"source":["-   the following `for` loop need to go through to $m$ cases to cover all data set as\n","\n","\\begin{equation}\\label{eq:for}\n","\\begin{split}\n","    &\\text{for j = 1 to m  }\\\\\n","& ~~~~~ z^{(i)} = w^Tx^{(i)} + b\\\\\n","& ~~~~~ a^{(i)} = \\sigma(z^{(i)})\\\\\n","& ~~~~~ J = (y^{(i)}\\log(\\hat{y}^{(i)})+(1-y^{(i)})\\log(1-\\hat{y}^{(i)})) \\\\\n","& ~~~~~ dz^{(i)} = a^{(i)}-y^{(i)}      \\\\\n","& ~~~~~ dw^{(i)} = x^{(i)} dz^{(i)}      \\\\\n","& ~~~~~ db^{(i)} = dz^{(i)}     \\\\\n","&J = J/m      \\\\\n","&dw = dw/m       \\\\\n","&db = db /m     \\\\\n","\\end{split}\n","\\end{equation}\n","\n","-   In the gradient descent loop\n","    -   both $ w $  and $b$ are updated based on gradient descent as\n","\n","\\begin{equation}\\label{gd}\n","\\begin{split}\n","w&= w - \\alpha~dw\\\\\n","b&= b - \\alpha~db\\\\\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6CRI7E0GFsb6"},"source":["#### Vectorized Logistic Regression\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TCf4GGDlFsb7"},"source":["-   by stacking this for loop horizontally into a vector, Logistic Regression can be vectorized as\n","\n","\\begin{equation}\\label{derivatesVector}\n","\\begin{split}\n","Z &= w^T X + b \\\\\n","A &= \\sigma(Z) \\\\\n","dZ &= A - Y\\\\\n","dW &= \\frac{1}{m} X~dZ^T \\\\\n","db &= \\frac{1}{m} \\sum_{i=1}^m  dZ\\\\\n","\\end{split}\n","\\end{equation}\n","\n","-   then the two `for` loops can be reduced to one `for` loop gradient descent as\n","\n","\\begin{equation}\\label{eq:forvec}\n","\\begin{split}\n","    &\\text{for j = 1 to maximum iteration  }\\\\\n","&~~~~~ Z = w^T X + b \\\\\n","&~~~~~ A = \\sigma(Z) \\\\\n","&~~~~~ dZ = A - Y\\\\\n","&~~~~~ dW = \\frac{1}{m} X~dZ^T \\\\\n","&~~~~~ db = \\frac{1}{m} \\sum_{i=1}^m  dZ\\\\\n","&~~~~~w = w - \\alpha~dw\\\\\n","&~~~~~b = b - \\alpha~db\\\\\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5ONkGgSoFsb7"},"source":["### Logistic Regression - Example\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lZsFrS0hFsb7"},"source":["#### Import python Libraries\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljKJ6CF3Fsb7","executionInfo":{"status":"aborted","timestamp":1647356404847,"user_tz":360,"elapsed":2173,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","import operator\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy\n","from PIL import Image\n","from scipy import ndimage"]},{"cell_type":"markdown","metadata":{"id":"If55k-H2Fsb7"},"source":["#### Import the data and show the top\n","\n"]},{"cell_type":"markdown","metadata":{"id":"elrgjzgqFsb7"},"source":["NOx over 200 ppm is high\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FKhwwW-Fsb7","executionInfo":{"status":"aborted","timestamp":1647356404847,"user_tz":360,"elapsed":2172,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["data = pd.read_csv('Engine_NOx_classification.csv')\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"laUESFi8Fsb7"},"source":["#### Set training and test data and plot training\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b525F-OrFsb8","executionInfo":{"status":"aborted","timestamp":1647356404847,"user_tz":360,"elapsed":2171,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["cdf = data[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'High NOx']]\n","\n","msk = np.random.rand(len(data)) < 0.8\n","train = cdf[msk]\n","test = cdf[~msk]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9qau6_wFsb8","executionInfo":{"status":"aborted","timestamp":1647356404848,"user_tz":360,"elapsed":2171,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["colors = {0: 'blue', 1:'red', 2:'green', 3:'coral', 4:'orange', 5:'black'}\n","\n","area = 300\n","# area = 200\n","plt.figure(1, figsize=(25, 6))\n","plt.scatter(train['Load [ft.lb]'], train['Engine speed [rpm]'], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"podpUmH5Fsb8"},"source":["#### Plot test data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5cQ-nijFsb8","executionInfo":{"status":"aborted","timestamp":1647356404848,"user_tz":360,"elapsed":2170,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["area = 300\n","# area = 200\n","plt.figure(1, figsize=(25, 6))\n","plt.scatter(test['Load [ft.lb]'], test['Engine speed [rpm]'], s=area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)\n","\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"0QbT5ExfFsb8"},"source":["### Logistic Regression classifier using equations\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Dye6XSIkFsb8"},"source":["#### Helper functions\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-PfBx0yRFsb8"},"source":["-   Using your code from \\`\\`Python Basics'', implement `sigmoid()`\n","-   As you've seen in the figure above, you need to compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use `np.exp()`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjrJt1urFsb8","executionInfo":{"status":"aborted","timestamp":1647356404848,"user_tz":360,"elapsed":2169,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def sigmoid(z):\n","    \"\"\"\n","    Compute the sigmoid of z\n","\n","    Arguments:\n","    z -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(z)\n","    \"\"\"\n","    s = 1/(1 + np.exp(-z))\n","    \n","    return s"]},{"cell_type":"markdown","metadata":{"id":"PgnPX529Fsb8"},"source":["#### Initializing parameters\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z54Rt47xFsb8"},"source":["-   parameter initialization is implemented in the cell below.\n","-   You have to initialize $w$ and $b$ as a vector of zeros.\n","-   this numpy function to use, check `np.zeros()` in the `Numpy` library's documentation.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UB80QFgTFsb9","executionInfo":{"status":"aborted","timestamp":1647356404849,"user_tz":360,"elapsed":2169,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def initialize_with_zeros(dim):\n","    \"\"\"\n","    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n","    \n","    Argument:\n","    dim -- size of the w vector we want (or number of parameters in this case)\n","    \n","    Returns:\n","    w -- initialized vector of shape (dim, 1)\n","    b -- initialized scalar (corresponds to the bias)\n","    \"\"\"\n","    \n","    w = np.zeros((dim,1))\n","    b = 0\n","    return w, b"]},{"cell_type":"markdown","metadata":{"id":"5t-mnL2MFsb9"},"source":["#### Forward and Backward propagation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y_JB0VW5Fsb9"},"source":["-   Now that your parameters are initialized, you can do the \\`\\`forward'' and \\`\\`backward'' propagation steps for learning the parameters.\n","\n","-   Implement a function `propagate()` that computes the cost function and its gradient.\n","-   Forward Propagation:\n","    -   You get $X$\n","    -   You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, \\dots , a^{(m-1)}, a^{(m)})$\n","    -   You calculate the cost function: $J = \\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n","-   these are the two equations you will be using:\n","\n","$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n","$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"14YooMZQFsb9"},"source":["#### Function propagate with gradient\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwZrACypFsb9","executionInfo":{"status":"aborted","timestamp":1647356404849,"user_tz":360,"elapsed":2167,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def propagate(w, b, X, Y):\n","    \"\"\"\n","    Implement the cost function and its gradient for the propagation explained above\n","\n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px * 3, number of examples)\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n","\n","    Return:\n","    cost -- negative log-likelihood cost for logistic regression\n","    dw -- gradient of the loss with respect to w, thus same shape as w\n","    db -- gradient of the loss with respect to b, thus same shape as b\n","    \n","    Tips:\n","    - Write your code step by step for the propagation. np.log(), np.dot()\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    \n","    # FORWARD PROPAGATION (FROM X TO COST)\n","    A = sigmoid(np.dot(w.T,X) + b)                                        # compute activation\n","    cost = (-1/m) * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A), axis=1)       # compute cost\n","    \n","    # BACKWARD PROPAGATION (TO FIND GRAD)\n","    dw = (1/m)*np.dot(X,(A-Y).T)\n","    db = (1/m)*np.sum((A-Y), axis=1)\n","\n","    cost = np.squeeze(cost)\n","\n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return grads, cost"]},{"cell_type":"markdown","metadata":{"id":"eLsnLzcBFsb9"},"source":["#### Optimization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u6q8NTozFsb9"},"source":["-   define the cost function\n","-   have initialized your parameters\n","-   so able to compute a cost function and its gradient\n","-   Now, update the parameters using gradient descent\n","-   The goal is to learn $w$ and $b$ by minimizing the cost function $J$\n","-   For a parameter $\\theta$, the update is $ \\theta = \\theta - \\alpha \\; d\\theta$\n","-   where $\\alpha$ is the learning rate.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gPvH6OZGFsb9"},"source":["#### Optimization Function\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NoHioyerFsb9","executionInfo":{"status":"aborted","timestamp":1647356404849,"user_tz":360,"elapsed":2166,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n","    \"\"\"\n","    This function optimizes w and b by running a gradient descent algorithm\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of shape (num_px * num_px * 3, number of examples)\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n","    num_iterations -- number of iterations of the optimization loop\n","    learning_rate -- learning rate of the gradient descent update rule\n","    print_cost -- True to print the loss every 100 steps\n","    \n","    Returns:\n","    params -- dictionary containing the weights w and bias b\n","    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n","    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n","    \n","    Tips:\n","    You basically need to write down two steps and iterate through them:\n","        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n","        2) Update the parameters using gradient descent rule for w and b.\n","    \"\"\"\n","    \n","    costs = []\n","    \n","    for i in range(num_iterations):\n","        \n","        \n","        # Cost and gradient calculation (approx 1-4 lines of code)\n","\n","        grads, cost = propagate(w, b, X, Y)\n","\n","        \n","        # Retrieve derivatives from grads\n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","        \n","        # update rule (approx 2 lines of code)\n","\n","        w = w - learning_rate*dw\n","        b = b - learning_rate*db\n","\n","        \n","        # Record the costs\n","        if i % 100 == 0:\n","            costs.append(cost)\n","        \n","        # Print the cost every 100 training iterations\n","        if print_cost and i % 100 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","    \n","    params = {\"w\": w,\n","              \"b\": b}\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return params, grads, costs"]},{"cell_type":"markdown","metadata":{"id":"rmYFcVvOFsb-"},"source":["#### Predict if the label is 0 or 1 using logistic regression\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lSZWmuCFsb-","executionInfo":{"status":"aborted","timestamp":1647356404849,"user_tz":360,"elapsed":2165,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def predict(w, b, X):\n","    '''\n","    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px * 3, number of examples)\n","    \n","    Returns:\n","    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n","    '''\n","    \n","    m = X.shape[1]\n","    Y_prediction = np.zeros((1,m))\n","    w = w.reshape(X.shape[0], 1)\n","    \n","    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n","\n","    A = sigmoid(np.dot(w.T,X) + b)  \n","\n","    \n","    for i in range(A.shape[1]):\n","        \n","        # Convert probabilities A[0,i] to actual predictions p[0,i]\n","\n","        if  A[0,i] < 0.5:\n","            Y_prediction[0, i] = 0\n","        else:\n","            Y_prediction[0, i] = 1\n","\n","\n","    \n","    return Y_prediction"]},{"cell_type":"markdown","metadata":{"id":"OjsON8HkFsb-"},"source":["#### Merge all functions into a model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LjclGoqLFsb-"},"source":["-   You will now see how the overall model is structured by\n","-   putting together all the building blocks\n","-   functions implemented above, in the right order.\n","-   Implement the model function. Use the following notation:\n","    -   Y<sub>prediction</sub><sub>test</sub> for your predictions on the test set\n","    -   Y<sub>prediction</sub><sub>train</sub> for your predictions on the train set\n","    -   w, costs, grads for the outputs of `optimize()`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"avjeuc8NFsb-"},"source":["#### Model Function -  builds the logistic regression model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otaSIbKgFsb-","executionInfo":{"status":"aborted","timestamp":1647356404850,"user_tz":360,"elapsed":2165,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n","    \"\"\"\n","    Builds the logistic regression model by calling the function you've implemented previously\n","    \n","    Arguments:\n","    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n","    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n","    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n","    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n","    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n","    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n","    print_cost -- Set to true to print the cost every 100 iterations\n","    \n","    Returns:\n","    d -- dictionary containing information about the model.\n","    \"\"\"\n","    \n","    \n","    # initialize parameters with zeros (approx 1 line of code)\n","    w, b = initialize_with_zeros(X_train.shape[0])\n","\n","    # Gradient descent (approx 1 line of code)\n","    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n","    \n","    # Retrieve parameters w and b from dictionary \"parameters\"\n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","    \n","    # Predict test/train set examples (approx 2 lines of code)\n","    Y_prediction_test = predict(w, b, X_test)\n","    Y_prediction_train = predict(w, b, X_train)\n","\n","\n","    # Print train/test Errors\n","    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n","\n","    \n","    d = {\"costs\": costs,\n","         \"Y_prediction_test\": Y_prediction_test, \n","         \"Y_prediction_train\" : Y_prediction_train, \n","         \"w\" : w, \n","         \"b\" : b,\n","         \"learning_rate\" : learning_rate,\n","         \"num_iterations\": num_iterations}\n","    \n","    return d"]},{"cell_type":"markdown","metadata":{"id":"1CWvje11Fsb-"},"source":["#### Run the training using above developed functions\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adnFG_gjFsb-","executionInfo":{"status":"aborted","timestamp":1647356404850,"user_tz":360,"elapsed":2164,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import preprocessing\n","train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]']])\n","train_y = np.asanyarray(train[['High NOx']])\n","\n","test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]']])\n","test_y = np.asanyarray(test[['High NOx']])\n","\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)\n","\n","d = model(X_train_minmax.T, train_y.T, X_test_minmax.T, test_y.T, num_iterations = 100000, learning_rate = 0.01, print_cost = True)"]},{"cell_type":"markdown","metadata":{"id":"ZDkAGlAmFsb-"},"source":["#### Plot learning curve (with costs)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9GAALyPFsb_","executionInfo":{"status":"aborted","timestamp":1647356404850,"user_tz":360,"elapsed":2163,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["costs = np.squeeze(d['costs'])\n","plt.plot(costs)\n","plt.ylabel('cost')\n","plt.xlabel('iterations (per hundreds)')\n","plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uiSLiafsFsb_"},"source":["#### Plot the classification of NOx as high or low - hand method\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNzwZmMsFsb_","executionInfo":{"status":"aborted","timestamp":1647356404850,"user_tz":360,"elapsed":2162,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["colors = {0: 'lightcoral', 1:'aqua', 2:'green', 3:'coral', 4:'orange', 5:'black'}\n","\n","X_plot = X_train_minmax.T\n","Y_plot = train_y.T\n","\n","X_plot_ts = X_test_minmax.T\n","Y_plot_ts = test_y.T\n","\n","# Set min and max values and give it some padding\n","x_min, x_max = X_plot[0, :].min() - 0.1, X_plot[0, :].max() + 0.1\n","y_min, y_max = X_plot[1, :].min() - 0.1, X_plot[1, :].max() + 0.1\n","h = 0.01\n","# Generate a grid of points with distance h between them\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","Z = predict(d[\"w\"],d[\"b\"],np.c_[xx.ravel().T, yy.ravel().T].T)\n","Z = Z.reshape(xx.shape)\n","# Plot the contour and training examples\n","plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","area = 30\n","plt.scatter(X_plot[0,:], X_plot[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","plt.scatter(X_plot_ts[0,:], X_plot_ts[1,:], s=8*area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)\n","\n","\n","\n","\n","predictions = predict(d[\"w\"],d[\"b\"], X_plot)\n","predictions_test = predict(d[\"w\"],d[\"b\"], X_plot_ts)\n","# accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.size)*100)\n","# accuracy_test = float((np.dot(Yts.T,predictions_test.T) + np.dot(1-Yts.T,1-predictions_test.T))/float(Yts.size)*100)\n","# print (\"Accuracy for train data: {} %\".format(accuracy))\n","# print (\"Accuracy for test data: {} %\".format(accuracy_test))"]},{"cell_type":"markdown","metadata":{"id":"nhZPzvs5Fsb_"},"source":["#### Logistic Regression classifier using sklearn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_kv4dEmFsb_","executionInfo":{"status":"aborted","timestamp":1647356404851,"user_tz":360,"elapsed":2162,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import preprocessing\n","\n","train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]']])\n","train_y = np.asanyarray(train[['High NOx']])\n","\n","test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]']])\n","test_y = np.asanyarray(test[['High NOx']])\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)"]},{"cell_type":"markdown","metadata":{"id":"QVvMVQfbFsb_"},"source":["#### Logistic regression and prediction\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0Gr4gQplFsb_"},"source":["-   import `LogisticRegression` from `sklearn`\n","-   the `penalty = 'l2'` is the type of regularization and `C` is $\\lambda$\n","-   show NOx classification for training data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fI8wz4fiFsb_","executionInfo":{"status":"aborted","timestamp":1647356404851,"user_tz":360,"elapsed":2161,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","clf = LogisticRegression(penalty='l2', C = 0.1, max_iter=1000).fit(X_train_minmax, train_y.ravel())\n","clf.predict(X_train_minmax)"]},{"cell_type":"markdown","metadata":{"id":"gxGZFuswFscA"},"source":["#### Plot the classification of NOx as high or low - sklearn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f3eKGQ3FscA","executionInfo":{"status":"aborted","timestamp":1647356404851,"user_tz":360,"elapsed":2160,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["colors = {0: 'lightcoral', 1:'aqua', 2:'green', 3:'coral', 4:'orange', 5:'black'}\n","\n","X = X_train_minmax\n","Y = train_y\n","\n","Xts = X_test_minmax\n","Yts = test_y\n","\n","X_plot = X_train_minmax.T\n","Y_plot = train_y.T\n","\n","X_plot_ts = X_test_minmax.T\n","Y_plot_ts = test_y.T\n","\n","# Set min and max values and give it some padding\n","x_min, x_max = X_plot[0, :].min() - 0.1, X_plot[0, :].max() + 0.1\n","y_min, y_max = X_plot[1, :].min() - 0.1, X_plot[1, :].max() + 0.1\n","h = 0.01\n","# Generate a grid of points with distance h between them\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","Z = clf.predict(np.c_[xx.ravel().T, yy.ravel().T])\n","Z = Z.reshape(xx.shape)\n","# Plot the contour and training examples\n","plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","area = 30\n","plt.scatter(X_plot[0,:], X_plot[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","plt.scatter(X_plot_ts[0,:], X_plot_ts[1,:], s=8*area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)\n","\n","\n","predictions = clf.predict(X)\n","predictions_test = clf.predict(Xts)\n","accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.size)*100)\n","accuracy_test = float((np.dot(Yts.T,predictions_test.T) + np.dot(1-Yts.T,1-predictions_test.T))/float(Yts.size)*100)\n","print (\"Accuracy for train data: {} %\".format(accuracy))\n","print (\"Accuracy for test data: {} %\".format(accuracy_test))"]},{"cell_type":"markdown","metadata":{"id":"bpNWym6WFscA"},"source":["### Unsupervised Learning k-means - background\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KNfp-nCfFscA"},"source":["#### Unsupervised Learning background\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uy9IGYxnFscA"},"source":["-   In unsupervised learning, the training data is unlabeled.\n","-   the algorithm system tries to learn without a solution\n","-   some of the most important unsupervised learning subsystems:\n","    -   Clustering\n","    -   Anomaly detection and novelty detection\n","    -   Association rule learning\n","-   where clustering among these play a crucial role, especially when the objective is to divide unlabeled data.\n","-   Spam detection of email is a well-known example of clustering using unsupervised learning.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Eu79gkDNFscA"},"source":["#### k-means\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QF3E0-3nFscA"},"source":["-   one of the simplest unsupervised models is k-means\n","-   Despite its simplicity, the K-means is widely used for clustering in many engineering applications\n","-   it is especially useful if you need to quickly gain insights from unlabeled data.\n","-   The way k-means algorithm works as follows:\n","    -   Specify number of clusters K\n","    -   Initialize centroids by first shuffling the dataset and then randomly selecting K data points for the centroids without replacement\n","    -   Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters is not changing\n","-   for a given $ m $ unlabeled data points and clustering to $ k $ cluster,\n","-   the cost function of k-means algorithm is defined as\n","\n","\\begin{equation}\n","    J = \\sum_{i = 1}^m \\sum_{k = 1}^K w_{ik}||x^i - \\mu_k||^2\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"06vTjhdTFscA"},"source":["#### k-means minimization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IgPZFwKbFscA"},"source":["-   it is a minimization problem of two parts.\n","-   first minimize $ J $ with respect to $ w_{ik} $ and treat $ \\mu_k$ fixed as\n","\n","\\begin{equation}\n","   \\frac{\\partial J}{\\partial w_{ik}}   = \\sum_{i = 1}^m \\sum_{k = 1}^K ||x^i - \\mu_k||^2 = 0\n","\\end{equation}\n","\n","which results in\n","\n","\\begin{equation}\\label{eq:relu}\n","\\begin{split}\n","w_{ik} &= \\begin{cases}\n","         1 & \\text{if } k = \\text{arg min} ||x^i - \\mu_k||^2 \\\\\n","         0 & \\text{otherwise } \\\\\n","    \\end{cases}\\\\\n","\\end{split}\n","\\end{equation}\n","\n","-   this assigns the data point $ x^i $ to the closest cluster, judged by its sum of squared distance from cluster centroid.\n","-   Then take the derivative of $ J $ with respect to $ \\mu_k $ and treat $ w_{ik}$ fixed.\n","-   This part is recompute the centroids after the cluster assignments from previous step as\n","\n","\\begin{equation}\n","   \\frac{\\partial J}{\\partial \\mu_k}   = 2 \\sum_{i = 1}^m \\sum_{k = 1}^K  w_{ik}(x^i - \\mu_k) = 0\n","\\end{equation}\n","\n","-   and results in\n","\n","\\begin{equation}\\label{eq:relu}\n","\\mu_k = \\frac{\\sum_{i = 1}^m w_{ik}x^i}{\\sum_{k = 1}^K w_{ik}}\n","\\end{equation}\n","\n","-   which translates to recomputing the centroid of each cluster to reflect the new assignments.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yCDcSjK_FscB"},"source":["#### k-means - initialize\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xHpX-FC3FscB"},"source":["-   Since clustering algorithms including k-means use distance-based measurements to determine the similarity between data points,\n","-   it is recommended to standardize the data to have a mean of zero and a standard deviation of one\n","    -   since the features in any dataset would typically have different units\n","    -   such as age vs income.\n","-   Given k-means iterative nature and the random initialization of centroids at the start of the algorithm,\n","-   different initializations may lead to different clusters since k-means algorithm may stuck in a local optimum and may not converge to global optimum.\n","-   Therefore, it is recommended to run the algorithm using different initializations of centroids and pick the results of the run that that results in the lowest sum of squared distance.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kEqOv7QZFscB"},"source":["#### Visualization for different groups\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aOaoPI2KFscB"},"source":["Animation of k-means for 3 clusters is shown schematically below (based on \\cite{scholkopf2002learning})\n","\n","![img](figures/K-means_convergence.gif \"SVM regression and support vectors, and the soft margin loss function for a linear SVM\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ndWDtJtPFscB"},"source":["### Unsupervised Learning K-means - simple python example\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r1MLeNfiFscB"},"source":["#### K means - simple example\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-LzN9B_oFscB"},"source":["##### import necessary libraries\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OClRTJi0FscB","executionInfo":{"status":"aborted","timestamp":1647356404851,"user_tz":360,"elapsed":2159,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","import operator\n","import matplotlib.pyplot as plt\n","get_ipython().run_line_magic('matplotlib', 'inline')"]},{"cell_type":"markdown","metadata":{"id":"6hzJvwn2FscB"},"source":["#### Simple example\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kDtiZWdeFscB"},"source":["-   import the data set\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_Pums-lFscB","executionInfo":{"status":"aborted","timestamp":1647356404852,"user_tz":360,"elapsed":2158,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.datasets.samples_generator import make_blobs"]},{"cell_type":"markdown","metadata":{"id":"O2odu3tJFscC"},"source":["##### Create the data set:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0u7DPf6KFscC"},"source":["-   First  up a random seed. Use `numpy's random.seed()` function, where the seed will be set to `0`\n","-   Next we will be making random clusters of points by using the `make_blobs` class.\n","-   The `make_blobs` class can take in many inputs, but we will be using these specific ones.\n","\n","<span class=\"underline\">Input</span>\n","\n","-   `n_samples` The total number of points equally divided among clusters.\n","    -   Value will be: 5000\n","-   `centers` The number of centers to generate, or the fixed center locations.\n","    -   Value will be: [[4, 4], [-2, -1], [2, -3],[1,1]]\n","-   `cluster_std` The standard deviation of the clusters.\n","    -   Value will be: 0.9\n","\n","<span class=\"underline\">Output</span>\n","\n","-   `X` : Array of shape [n<sub>samples</sub>, n<sub>features</sub>]. (Feature Matrix)\n","    -   The generated samples.\n","-   `y` : Array of shape [n<sub>samples</sub>]. (Response Vector)\n","    -   The integer labels for cluster membership of each sample.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vhztU21ZFscC"},"source":["#### Scatter plot of the randomly generated data.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFr-XxkYFscC","executionInfo":{"status":"aborted","timestamp":1647356404852,"user_tz":360,"elapsed":2157,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["np.random.seed(0)\n","X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9)\n","plt.scatter(X[:, 0], X[:, 1], marker='.')"]},{"cell_type":"markdown","metadata":{"id":"S9E2dAVAFscC"},"source":["#### Setting up K-Means\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aV9yD1nZFscC"},"source":["-   Using the random data setup K-Means Clustering.\n","-   The k-means class has many parameters that can be used, but we will be using these three:\n","-   `init` : Initialization method of the centroids. \n","    -   Value will be: `k-means++`\n","    -   `k-means++` : Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n","-   `n_clusters` : The number of clusters to form as well as the number of centroids to generate. \n","    -   Value will be: 4 (since we have 4 centers)\n","    -   `n_init` : Number of time the k-means algorithm will be run with different centroid seeds.\n","    -   The final results will be the best output of `n_init` consecutive runs in terms of inertia.\n","-   Choose a value will be: 12\n","-   Initialize k-means with these parameters, where the output parameter is called `k_means`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cGLFq3DFscC","executionInfo":{"status":"aborted","timestamp":1647356404852,"user_tz":360,"elapsed":2156,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.cluster import KMeans\n","k_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)"]},{"cell_type":"markdown","metadata":{"id":"X7-rqZ27FscC"},"source":["#### Fit the k-means model with the data matrix X\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY1XtJWoFscC","executionInfo":{"status":"aborted","timestamp":1647356404853,"user_tz":360,"elapsed":2156,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["k_means.fit(X)"]},{"cell_type":"markdown","metadata":{"id":"07JpffRYFscC"},"source":["-   Now let's obtain the labels for each point in the model using the label attribute `k_means.labels`\n","-   and save it as `k_means_labels`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6iFIEKn3FscD","executionInfo":{"status":"aborted","timestamp":1647356404853,"user_tz":360,"elapsed":2155,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["k_means_labels = k_means.labels_\n","k_means_labels"]},{"cell_type":"markdown","metadata":{"id":"AvnbX1NqFscD"},"source":["-   get the coordinates of the cluster centers using `k_means.cluster_centers_`\n","-   and save it as `k_means_cluster_centers`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y4UmsamBFscD","executionInfo":{"status":"aborted","timestamp":1647356404854,"user_tz":360,"elapsed":2155,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["k_means_cluster_centers = k_means.cluster_centers_\n","k_means_cluster_centers"]},{"cell_type":"markdown","metadata":{"id":"jIdjdchbFscD"},"source":["#### Visualize with a Plot\n","\n"]},{"cell_type":"markdown","metadata":{"id":"84acDdmCFscD"},"source":["-   have the random data generated and the k-means model initialized\n","-   can now plot them\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1qo86GIFscD","executionInfo":{"status":"aborted","timestamp":1647356404855,"user_tz":360,"elapsed":2155,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["# Initialize the plot with the specified dimensions.\n","fig = plt.figure(figsize=(6, 4))\n","\n","# Colors uses a color map, which will produce an array of colors based on\n","# the number of labels there are. We use set(k_means_labels) to get the\n","# unique labels.\n","colors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels))))\n","\n","# Create a plot\n","ax = fig.add_subplot(1, 1, 1)\n","\n","# For loop that plots the data points and centroids.\n","# k will range from 0-3, which will match the possible clusters that each\n","# data point is in.\n","for k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1]])), colors):\n","\n","    # Create a list of all data points, where the data poitns that are \n","    # in the cluster (ex. cluster 0) are labeled as true, else they are\n","    # labeled as false.\n","    my_members = (k_means_labels == k)\n","    \n","    # Define the centroid, or cluster center.\n","    cluster_center = k_means_cluster_centers[k]\n","    \n","    # Plots the datapoints with color col.\n","    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.')\n","    \n","    # Plots the centroids with specified color, but with a darker outline\n","    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)\n","\n","# Title of the plot\n","ax.set_title('KMeans - Simple Example')\n","\n","# Remove x-axis ticks\n","ax.set_xticks(())\n","\n","# Remove y-axis ticks\n","ax.set_yticks(())\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XhMIG6FsFscD"},"source":["### Unsupervised Learning K-means - Diesel engine example\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GbeX1SyaFscD"},"source":["#### Diesel engine - NOx as a function of load\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SZ1FFJuOFscD"},"source":["-   Import the data and show the first lines\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZmO-6I1FscD","executionInfo":{"status":"aborted","timestamp":1647356404855,"user_tz":360,"elapsed":2154,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["data = pd.read_csv('Engine_NOx.csv')\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"xZ393vn-FscE"},"source":["#### Plot the data - a feature is NOx\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLzYmBzmFscE","executionInfo":{"status":"aborted","timestamp":1647356404856,"user_tz":360,"elapsed":2154,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.scatter(data['Load [ft.lb]'], data['NOx [ppm]'],  color='blue')\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9FOdv3tdFscE"},"source":["#### Split the data into training 80% and test 20%\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fW72uUGQFscE","executionInfo":{"status":"aborted","timestamp":1647356404856,"user_tz":360,"elapsed":2153,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["cdf = data[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'NOx [ppm]']]\n","\n","msk = np.random.rand(len(data)) < 0.8\n","train = cdf[msk]\n","test = cdf[~msk]"]},{"cell_type":"markdown","metadata":{"id":"hyT-3n8aFscE"},"source":["-   plot training and test data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHlmvbuWFscE","executionInfo":{"status":"aborted","timestamp":1647356404856,"user_tz":360,"elapsed":2151,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.scatter(train['Load [ft.lb]'], train['NOx [ppm]'],  color='blue')\n","plt.scatter(test['Load [ft.lb]'], test['NOx [ppm]'],  color='red')\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jwHpFJsAFscE"},"source":["#### NOx first attempt: try 4 clusters, cluster based on load, speed, and NOx\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EIH1nfugFscE"},"source":["-   Prepare the data for sklearn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Th4kjIVFscE","executionInfo":{"status":"aborted","timestamp":1647356404856,"user_tz":360,"elapsed":2150,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import preprocessing\n","\n","train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]', 'NOx [ppm]']])\n","test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]', 'NOx [ppm]']])\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)"]},{"cell_type":"markdown","metadata":{"id":"0n3RRSxVFscF"},"source":["#### NOx first attempt: run k-means\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uI4rD3XeFscF","executionInfo":{"status":"aborted","timestamp":1647356404857,"user_tz":360,"elapsed":2150,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.cluster import KMeans \n","k_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)\n","k_means.fit(train_x)"]},{"cell_type":"markdown","metadata":{"id":"edbvtq0cFscF"},"source":["-   look at the labels\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpyc8krCFscF","executionInfo":{"status":"aborted","timestamp":1647356404857,"user_tz":360,"elapsed":2148,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["k_means_labels = k_means.labels_\n","k_means_labels"]},{"cell_type":"markdown","metadata":{"id":"6bgV7FDuFscF"},"source":["-   look at the center of the clusters\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xa4YBfQFscF","executionInfo":{"status":"aborted","timestamp":1647356404857,"user_tz":360,"elapsed":2147,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["k_means_cluster_centers = k_means.cluster_centers_\n","k_means_cluster_centers"]},{"cell_type":"markdown","metadata":{"id":"r852JpebFscF"},"source":["#### NOx first attempt: plot the training data and cluster\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwB2786NFscF","executionInfo":{"status":"aborted","timestamp":1647356404858,"user_tz":360,"elapsed":2147,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["X = train_x\n","\n","area = 0.00001*np.pi * ( X[:, 1])**2  \n","plt.scatter(train['Load [ft.lb]'], train['NOx [ppm]'], s=area, c=k_means_labels.astype(np.float), alpha=0.5)\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"r5jCQKP_FscF"},"source":["#### NOx second attempt: try 4 clusters, cluster based on load, speed, injected fuel amount, common pressure and NOx\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5AqRbRHjFscF"},"source":["-   now have 4 features\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lp-USmnAFscF","executionInfo":{"status":"aborted","timestamp":1647356404858,"user_tz":360,"elapsed":2146,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'NOx [ppm]']])\n","test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'NOx [ppm]']])\n","\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)\n","\n","k_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)\n","k_means.fit(train_x)\n","\n","k_means_labels = k_means.labels_\n","k_means_labels"]},{"cell_type":"markdown","metadata":{"id":"UIwK8JLaFscG"},"source":["#### NOx second attempt: plot the training data and cluster\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzT0zYb7FscG","executionInfo":{"status":"aborted","timestamp":1647356404858,"user_tz":360,"elapsed":2145,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["X = train_x\n","\n","area = 0.00001*np.pi * ( X[:, 1])**2  \n","plt.scatter(train['Load [ft.lb]'], train['NOx [ppm]'], s=area, c=k_means_labels.astype(np.float), alpha=0.5)\n","plt.xlabel(\"Load [ft.lb]\")\n","plt.ylabel(\"NOx [ppm]\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WCnJFuSrFscG"},"source":["#### Find the best value of k\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z_923QS9FscG"},"source":["-   try several cases to find the best number for K and plot\n","-   **Inertia:** Sum of squared distances of samples to their closest cluster center.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Fk5FsU-FscG","executionInfo":{"status":"aborted","timestamp":1647356404858,"user_tz":360,"elapsed":2144,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["inertia_save = []\n","for i in range(20):\n","    clusterNum = i + 1\n","    k_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12)\n","    k_means.fit(X)\n","    a = k_means.inertia_\n","    inertia_save.append(a)\n","\n","plt.figure(1, figsize=(14, 5))\n","plt.plot(inertia_save,'-o')\n","plt.plot(6,inertia_save[6],'-o', markersize=12)\n","plt.ylabel('Inertia', fontsize=18)\n","plt.xlabel('K in K-means', fontsize=16)\n","plt.xticks(np.arange(0, 20, 2))\n","plt.yscale('log')"]},{"cell_type":"markdown","metadata":{"id":"ZVFEZwUTFscG"},"source":["#### :PROPERTIES:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mdWo3yVkFscG"},"source":["\\printbibliography\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"org":null,"colab":{"name":"L2_2021v2h.ipynb","provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}