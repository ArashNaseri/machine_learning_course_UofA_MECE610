{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNqofmrZ5Ny7zvtVjKhyXtG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["   # Natural Language Processing using TensorFlow\n","   \n","   - Developed by **Armin Norouzi**\n","   - Compatible with Google Colaboratory- Tensorflow 2.8.2\n","\n","   \n","   - **Objective:** Using pretrained model and perfrom different kinds of transfer learning\n","   \n","\n","   \n","**Table of content:**\n","\n","1. Introduction to Natural Language Processing (NLP)\n","\n","\n","\n"],"metadata":{"id":"Hc5RiwiU-4WM"}},{"cell_type":"markdown","source":["## Introduction to Trasnfer Learning (NLP)"],"metadata":{"id":"YmoM5bVxmXAX"}},{"cell_type":"markdown","source":["The main goal of [natural language processing (NLP)](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32) is to derive information from natural language.\n","\n","Natural language is a broad term but you can consider it to cover any of the following:\n","* Text (such as that contained in an email, blog post, book, Tweet)\n","* Speech (a conversation you have with a doctor, voice commands you give to a smart speaker)\n","\n","Under the umbrellas of text and speech there are many different things you might want to do.\n"],"metadata":{"id":"aWafGPEt3h-y"}},{"cell_type":"markdown","source":["### Helper Function\n","\n","These functions developed in previous lectures"],"metadata":{"id":"LSMg5LWz4bC3"}},{"cell_type":"code","source":["# Create function to unzip a zipfile into current working directory \n","# (since we're going to be downloading and unzipping a few files)\n","import zipfile\n","\n","def unzip_data(filename):\n","  \"\"\"\n","  Unzips filename into the current working directory.\n","  Args:\n","    filename (str): a filepath to a target zip folder to be unzipped.\n","  \"\"\"\n","  zip_ref = zipfile.ZipFile(filename, \"r\")\n","  zip_ref.extractall()\n","  zip_ref.close()"],"metadata":{"id":"MuwAOCIx3hgC","executionInfo":{"status":"ok","timestamp":1662578449239,"user_tz":240,"elapsed":152,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import datetime\n","\n","def create_tensorboard_callback(dir_name, experiment_name):\n","  \"\"\"\n","  Creates a TensorBoard callback instand to store log files.\n","  Stores log files with the filepath:\n","    \"dir_name/experiment_name/current_datetime/\"\n","  Args:\n","    dir_name: target directory to store TensorBoard log files\n","    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n","  \"\"\"\n","  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n","      log_dir=log_dir\n","  )\n","  print(f\"Saving TensorBoard log files to: {log_dir}\")\n","  return tensorboard_callback"],"metadata":{"id":"0HzGAcy83hdC","executionInfo":{"status":"ok","timestamp":1662578449372,"user_tz":240,"elapsed":5,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Plot the validation and training data separately\n","import matplotlib.pyplot as plt\n","\n","def plot_loss_curves(history):\n","  \"\"\"\n","  Returns separate loss curves for training and validation metrics.\n","  Args:\n","    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n","  \"\"\" \n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  accuracy = history.history['accuracy']\n","  val_accuracy = history.history['val_accuracy']\n","\n","  epochs = range(len(history.history['loss']))\n","\n","  # Plot loss\n","  plt.plot(epochs, loss, label='training_loss')\n","  plt.plot(epochs, val_loss, label='val_loss')\n","  plt.title('Loss')\n","  plt.xlabel('Epochs')\n","  plt.legend()\n","\n","  # Plot accuracy\n","  plt.figure()\n","  plt.plot(epochs, accuracy, label='training_accuracy')\n","  plt.plot(epochs, val_accuracy, label='val_accuracy')\n","  plt.title('Accuracy')\n","  plt.xlabel('Epochs')\n","  plt.legend();"],"metadata":{"id":"9khSwzv23haK","executionInfo":{"status":"ok","timestamp":1662578449372,"user_tz":240,"elapsed":4,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def compare_historys(original_history, new_history, initial_epochs=5):\n","    \"\"\"\n","    Compares two TensorFlow model History objects.\n","    \n","    Args:\n","      original_history: History object from original model (before new_history)\n","      new_history: History object from continued model training (after original_history)\n","      initial_epochs: Number of epochs in original_history (new_history plot starts from here) \n","    \"\"\"\n","    \n","    # Get original history measurements\n","    acc = original_history.history[\"accuracy\"]\n","    loss = original_history.history[\"loss\"]\n","\n","    val_acc = original_history.history[\"val_accuracy\"]\n","    val_loss = original_history.history[\"val_loss\"]\n","\n","    # Combine original history with new history\n","    total_acc = acc + new_history.history[\"accuracy\"]\n","    total_loss = loss + new_history.history[\"loss\"]\n","\n","    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n","    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n","\n","    # Make plots\n","    plt.figure(figsize=(8, 8))\n","    plt.subplot(2, 1, 1)\n","    plt.plot(total_acc, label='Training Accuracy')\n","    plt.plot(total_val_acc, label='Validation Accuracy')\n","    plt.plot([initial_epochs-1, initial_epochs-1],\n","              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n","    plt.legend(loc='lower right')\n","    plt.title('Training and Validation Accuracy')\n","\n","    plt.subplot(2, 1, 2)\n","    plt.plot(total_loss, label='Training Loss')\n","    plt.plot(total_val_loss, label='Validation Loss')\n","    plt.plot([initial_epochs-1, initial_epochs-1],\n","              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n","    plt.legend(loc='upper right')\n","    plt.title('Training and Validation Loss')\n","    plt.xlabel('epoch')\n","    plt.show()\n","  "],"metadata":{"id":"RyPBrBAS3hXi","executionInfo":{"status":"ok","timestamp":1662578449372,"user_tz":240,"elapsed":4,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Load and prepare data"],"metadata":{"id":"k9AvF7VqIn0W"}},{"cell_type":"markdown","source":["### Download data"],"metadata":{"id":"fvEAxfODNI6e"}},{"cell_type":"markdown","source":["\n","\n","Let's start by download a text dataset. We'll be using the [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) datset from Kaggle which contains text-based Tweets about natural disasters. The original downloaded data has not been altered to how you would download it from Kaggle:\n","\n","* `sample_submission.csv` - an example of the file you'd submit to the Kaggle competition of your model's predictions.\n","* `train.csv` - training samples of real and not real diaster Tweets.\n","* `test.csv` - testing samples of real and not real diaster Tweets."],"metadata":{"id":"S1UkOUOvInlW"}},{"cell_type":"code","source":["# Turn .csv files into pandas DataFrame's\n","import pandas as pd\n","train_df = pd.read_csv(\"https://raw.githubusercontent.com/arminnorouzi/machine_learning_course_UofA_MECE610/main/Data/NLP_train.csv\")\n","train_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"66RSpEPKJ0PM","executionInfo":{"status":"ok","timestamp":1662578449791,"user_tz":240,"elapsed":422,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"bd5ec7d7-0d11-41fa-8441-6bef048d75fd"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id keyword location                                               text  \\\n","0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n","1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n","2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n","3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n","4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n","\n","   target  \n","0       1  \n","1       1  \n","2       1  \n","3       1  \n","4       1  "],"text/html":["\n","  <div id=\"df-84efb15b-075d-4414-bc9f-c024f8a8e9f1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84efb15b-075d-4414-bc9f-c024f8a8e9f1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-84efb15b-075d-4414-bc9f-c024f8a8e9f1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-84efb15b-075d-4414-bc9f-c024f8a8e9f1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["test_df = pd.read_csv(\"https://raw.githubusercontent.com/arminnorouzi/machine_learning_course_UofA_MECE610/main/Data/NLP_test.csv\")\n","test_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"roDE7Jaa3hVC","executionInfo":{"status":"ok","timestamp":1662578449983,"user_tz":240,"elapsed":195,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"47ddb729-08e6-4204-d5f1-60a375a5a9c7"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id keyword location                                               text\n","0   0     NaN      NaN                 Just happened a terrible car crash\n","1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n","2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n","3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n","4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"],"text/html":["\n","  <div id=\"df-2e57b5e2-de9c-41c5-a9c7-5da9674d90e6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just happened a terrible car crash</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Heard about #earthquake is different cities, s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>there is a forest fire at spot pond, geese are...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Apocalypse lighting. #Spokane #wildfires</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e57b5e2-de9c-41c5-a9c7-5da9674d90e6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2e57b5e2-de9c-41c5-a9c7-5da9674d90e6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2e57b5e2-de9c-41c5-a9c7-5da9674d90e6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["Suffling dataset"],"metadata":{"id":"ie7U_vX8KQvF"}},{"cell_type":"code","source":["# Shuffle training dataframe\n","train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n","train_df_shuffled.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"8oLjwjfr3hSK","executionInfo":{"status":"ok","timestamp":1662578449984,"user_tz":240,"elapsed":8,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"ea6f0596-ef08-4c0b-e381-458c053b45cf"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        id      keyword               location  \\\n","2644  3796  destruction                    NaN   \n","2227  3185       deluge                    NaN   \n","5448  7769       police                     UK   \n","132    191   aftershock                    NaN   \n","6845  9810       trauma  Montgomery County, MD   \n","\n","                                                   text  target  \n","2644  So you have a new weapon that can cause un-ima...       1  \n","2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n","5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n","132   Aftershock back to school kick off was great. ...       0  \n","6845  in response to trauma Children of Addicts deve...       0  "],"text/html":["\n","  <div id=\"df-4d3a9165-9178-487b-b457-a9b019c7f49b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2644</th>\n","      <td>3796</td>\n","      <td>destruction</td>\n","      <td>NaN</td>\n","      <td>So you have a new weapon that can cause un-ima...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2227</th>\n","      <td>3185</td>\n","      <td>deluge</td>\n","      <td>NaN</td>\n","      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5448</th>\n","      <td>7769</td>\n","      <td>police</td>\n","      <td>UK</td>\n","      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>132</th>\n","      <td>191</td>\n","      <td>aftershock</td>\n","      <td>NaN</td>\n","      <td>Aftershock back to school kick off was great. ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6845</th>\n","      <td>9810</td>\n","      <td>trauma</td>\n","      <td>Montgomery County, MD</td>\n","      <td>in response to trauma Children of Addicts deve...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d3a9165-9178-487b-b457-a9b019c7f49b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4d3a9165-9178-487b-b457-a9b019c7f49b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4d3a9165-9178-487b-b457-a9b019c7f49b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# How many examples of each class?\n","train_df.target.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4I2liHi3hPd","executionInfo":{"status":"ok","timestamp":1662578449985,"user_tz":240,"elapsed":7,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"72922251-2fa4-45c9-dfc1-10666078c6ce"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    4342\n","1    3271\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["print(\"Percentage of negetive class:\", round(100*4342/(3271 + 4342),2))\n","print(\"Percentage of positive class:\", round(100*3271/(3271 + 4342),2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JdfzVoZUKi5d","executionInfo":{"status":"ok","timestamp":1662578450150,"user_tz":240,"elapsed":9,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"4278fced-f995-46e4-abf7-a527d23e3fde"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Percentage of negetive class: 57.03\n","Percentage of positive class: 42.97\n"]}]},{"cell_type":"markdown","source":["Since we have two target values, we're dealing with a **binary classification** problem.\n","\n","It's fairly balanced too, about 57% negative class (`target = 0`) and 43% positive class (`target = 1`).\n","\n","Where, \n","\n","* `1` = a real disaster Tweet\n","* `0` = not a real disaster Tweet\n","\n","And what about the total number of samples we have?"],"metadata":{"id":"ng2g1B7JKhE9"}},{"cell_type":"code","source":["print(\"Percentage of training data:\", round(100*len(train_df)/(len(train_df) + len(test_df)), 2))\n","print(\"Percentage of testing data:\", round(100*len(test_df)/(len(train_df) + len(test_df)), 2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWoYd5Eo3hMl","executionInfo":{"status":"ok","timestamp":1662578450150,"user_tz":240,"elapsed":7,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"0d7cd5fd-9f97-41db-ef08-82f2f67b34fe"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Percentage of training data: 70.0\n","Percentage of testing data: 30.0\n"]}]},{"cell_type":"code","source":["# Let's visualize some random training examples\n","import random\n","random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n","for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index + 5].itertuples():\n","  _, text, target = row\n","  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n","  print(f\"Text:\\n{text}\\n\")\n","  print(\"---\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nXaEV58r3hJy","executionInfo":{"status":"ok","timestamp":1662578450150,"user_tz":240,"elapsed":4,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"15534c32-33a2-4e18-a7ac-e736eeb6d352"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Target: 0 (not real disaster)\n","Text:\n","@chriscesq The average GOP voter would go to a big-tent circus in a hailstorm/windstorm no? :-)\n","\n","---\n","\n","Target: 0 (not real disaster)\n","Text:\n","STL Ace Grille - Surface Mounts SpeedTech Lights - Amber Emergency Lights - 544 http://t.co/t6Seku4yvm http://t.co/TJOZ4u4txl\n","\n","---\n","\n","Target: 0 (not real disaster)\n","Text:\n","@gemmahentsch @megynkelly @DLoesch I can not envision any catastrophe that would prevent a woman placing her child for adoption.\n","\n","---\n","\n","Target: 1 (real disaster)\n","Text:\n","The Latest: More homes razed by Northern California wildfire - LancasterOnline http://t.co/ph7wllKRfI #Lancaster\n","\n","---\n","\n","Target: 1 (real disaster)\n","Text:\n","@FoxNews He still has his beard - has he been visited by any1 while in prison? If he keeps that hideous beard electrocute him! \n","#UglyPeople\n","\n","---\n","\n"]}]},{"cell_type":"markdown","source":["### Split data into training and validation sets"],"metadata":{"id":"GLI6PWegM54b"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Use train_test_split to split training data into training and validation sets\n","train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n","                                                                            train_df_shuffled[\"target\"].to_numpy(),\n","                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n","                                                                            random_state=42) # random state for reproducibility"],"metadata":{"id":"U2V4SI4I3hHa","executionInfo":{"status":"ok","timestamp":1662578450447,"user_tz":240,"elapsed":299,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# How many samples total?\n","print(f\"Total training samples: {len(train_sentences)}\")\n","print(f\"Total validation samples: {len(val_sentences)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JisRCr9P3hES","executionInfo":{"status":"ok","timestamp":1662578450447,"user_tz":240,"elapsed":7,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"ff8d093b-33ce-46c2-b0fb-32795e41bb6f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Total training samples: 6851\n","Total validation samples: 762\n"]}]},{"cell_type":"markdown","source":["## Converting text into numbers\n"],"metadata":{"id":"KcWLqGg9T8dL"}},{"cell_type":"markdown","source":["In NLP, there are two main concepts for turning text into numbers:\n","* **Tokenization:** A straight mapping from word or character or sub-word to a numerical value. \n","  1. **Word-level tokenization:** In this case, every word in a sequence considered a single *token*. With the sentence \"I love TensorFlow\" might result in \"I\" being `0`, \"love\" being `1` and \"TensorFlow\" being `2`\n","  2. **Character-level tokenization:** In this case, every character in a sequence considered a single *token*.\n","  3. **Sub-word tokenization:** This method is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple *tokens*.\n"],"metadata":{"id":"bxZr0vYBT7QU"}},{"cell_type":"markdown","source":["* **Embeddings:** An embedding is a representation of natural language which can be learned. Representation comes in the form of a *feature vector*. For example, the word \"dance\" could be represented by the 5-dimensional vector `[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]`. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings: \n","  1. **Create your own embedding:** Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)) and an embedding representation will be learned during model training.\n","  2. **Reuse a pre-learned embedding:** Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task. Good model to start:\n","    - [Word2vec embeddings](https://jalammar.github.io/illustrated-word2vec/)\n","    - [GloVe embeddings](https://nlp.stanford.edu/projects/glove/)\n","\n"],"metadata":{"id":"ubgLO9MiW8pr"}},{"cell_type":"markdown","source":["### Text vectorization (tokenization)"],"metadata":{"id":"-P1qcG5mXad7"}},{"cell_type":"markdown","source":["To tokenize our words, we'll use the helpful preprocessing layer [`tf.keras.layers.experimental.preprocessing.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n","\n","The `TextVectorization` layer takes the following parameters:\n","* `max_tokens`: The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens. \n","* `standardize`: Method for standardizing text. Default is `\"lower_and_strip_punctuation\"` which lowers text and removes all punctuation marks.\n","* `split`: How to split text, default is `\"whitespace\"` which splits on spaces.\n","* `ngrams`: How many words to contain per token split, for example, `ngrams=2` splits tokens into continuous sequences of 2.\n","* `output_mode`: How to output tokens, can be `\"int\"` (integer mapping), `\"binary\"` (one-hot encoding), `\"count\"` or `\"tf-idf\"`. See documentation for more.\n","* `output_sequence_length`: Length of tokenized sequence to output. For example, if `output_sequence_length=150`, all tokenized sequences will be 150 tokens long.\n","* `pad_to_max_tokens`: Defaults to `False`, if `True`, the output feature axis will be padded to `max_tokens` even if the number of unique tokens in the vocabulary is less than `max_tokens`. Only valid in certain modes, see docs for more."],"metadata":{"id":"-PPq2ekZXdzD"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","\n","# Use the default TextVectorization variables\n","text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n","                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n","                                    split=\"whitespace\", # how to split tokens\n","                                    ngrams=None, # create groups of n-words?\n","                                    output_mode=\"int\", # how to map tokens to numbers\n","                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n","                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"],"metadata":{"id":"At74NBpy3hBf","executionInfo":{"status":"ok","timestamp":1662578450447,"user_tz":240,"elapsed":4,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["This `text_vectorizer` is based on default settings. "],"metadata":{"id":"S2VvE1qWY15q"}},{"cell_type":"markdown","source":["For `max_tokens` (the number of words in the vocabulary), multiples of 10,000 (`10,000`, `20,000`, `30,000`) or the exact number of unique words in your text (e.g. `10,876`) are common values. For our use case, we'll use `10,000`."],"metadata":{"id":"76hOHFiwZMNq"}},{"cell_type":"markdown","source":["And for the `output_sequence_length` we can use the average number of tokens per Tweet in the training set:"],"metadata":{"id":"Bnwh5JoUZWGr"}},{"cell_type":"code","source":["# Find average number of tokens (words) in training Tweets\n","round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5nyg6oQ3g-q","executionInfo":{"status":"ok","timestamp":1662578450447,"user_tz":240,"elapsed":4,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"77f4bdb1-a017-4987-fb77-80a82851bed4"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["Now let's create another `TextVectorization` object using our custom parameters."],"metadata":{"id":"vsW1Ua5wZ0-i"}},{"cell_type":"code","source":["# Setup text vectorization with custom variables\n","max_vocab_length = 10000 # max number of words to have in our vocabulary\n","max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n","\n","text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n","                                    output_mode=\"int\",\n","                                    output_sequence_length=max_length)"],"metadata":{"id":"AjgG-A_g3g8K","executionInfo":{"status":"ok","timestamp":1662578548595,"user_tz":240,"elapsed":145,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["To map our `TextVectorization` instance `text_vectorizer` to our data, we can call the `adapt()` method on it whilst passing it our training text."],"metadata":{"id":"DO4TrZUaZ8xK"}},{"cell_type":"code","source":["# Fit the text vectorizer to the training text\n","text_vectorizer.adapt(train_sentences)"],"metadata":{"id":"UW7_uw9x3g5a","executionInfo":{"status":"ok","timestamp":1662578585512,"user_tz":240,"elapsed":655,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# For visualization purpose\n","# Choose a random sentence from the training dataset and tokenize it\n","random_sentence = random.choice(train_sentences)\n","print(f\"Original text:\\n{random_sentence}\\\n","      \\n\\nVectorized version:\")\n","text_vectorizer([random_sentence])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSNi5OJW3g2q","executionInfo":{"status":"ok","timestamp":1662578632928,"user_tz":240,"elapsed":153,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"c4c9b243-3ef6-4a81-d85e-0dd067c8c515"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text:\n","Daily Reflections\n","August 6\n","DRIVEN\n","Driven by a hundred forms of fear self-delusion self-seeking and self-pity... http://t.co/DXfqOu4kT2      \n","\n","Vectorized version:\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n","array([[ 857, 9249,  407,  560, 3046, 3046,   18,    3, 3761,    1,    6,\n","         201, 8677, 8672,    7]])>"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["Finally, we can check the unique tokens in our vocabulary using the `get_vocabulary()` method. This method returns array sorted by frequency of each token!"],"metadata":{"id":"238fk6fXazMu"}},{"cell_type":"code","source":["# Get the unique words in the vocabulary\n","words_in_vocab = text_vectorizer.get_vocabulary()\n","top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n","bottom_5_words = words_in_vocab[-5:] # least common tokens\n","print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n","print(f\"Top 5 most common words: {top_5_words}\") \n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkn7e5GJ3gzy","executionInfo":{"status":"ok","timestamp":1662579203490,"user_tz":240,"elapsed":185,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"c671a548-ea5b-4e6b-e270-292a9fe4cdbd"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of words in vocab: 10000\n","Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n","Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"]}]},{"cell_type":"markdown","source":["### Creating an Embedding using an Embedding Layer"],"metadata":{"id":"mCGKXGNlcYq6"}},{"cell_type":"markdown","source":["The powerful thing about an embedding is it can be learned during training. This means rather than just being static (e.g. `1` = I, `2` = love, `3` = TensorFlow), a word's numeric representation can be improved as a model goes through data samples.\n","\n","We can see what an embedding of a word looks like by using the [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer. \n","\n","The main parameters we're concerned about here are:\n","* `input_dim`: The size of the vocabulary (e.g. `len(text_vectorizer.get_vocabulary()`).\n","* `output_dim`: The size of the output embedding vector, for example, a value of `100` outputs a  feature vector of size 100 for each word.\n","* `embeddings_initializer`: How to initialize the embeddings matrix, default is `\"uniform\"` which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n","* `input_length`: Length of sequences being passed to embedding layer.\n","\n","Knowing these, let's make an embedding layer."],"metadata":{"id":"fv80lDHHdDIO"}},{"cell_type":"code","source":["tf.random.set_seed(42)\n","from tensorflow.keras import layers\n","\n","embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n","                             output_dim=128, # set size of embedding vector\n","                             embeddings_initializer=\"uniform\", # default, intialize randomly\n","                             input_length=max_length, # how long is each input\n","                             name=\"embedding_1\") \n","\n","embedding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i8g1Lh343guT","executionInfo":{"status":"ok","timestamp":1662579735522,"user_tz":240,"elapsed":133,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"62e2a9c3-1669-4bba-b9da-269629b5db78"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.layers.embeddings.Embedding at 0x7f572d3c3290>"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["Notice how `embedding` is a TensoFlow layer? This is important because we can use it as part of a model, meaning its parameters (word representations) can be updated and improved as the model learns"],"metadata":{"id":"hbtPQZ82efyX"}},{"cell_type":"code","source":["# Get a random sentence from training set\n","random_sentence = random.choice(train_sentences)\n","print(f\"Original text:\\n{random_sentence}\\\n","      \\n\\nEmbedded version:\")\n","\n","# Embed the random sentence (turn it into numerical representation)\n","sample_embed = embedding(text_vectorizer([random_sentence]))\n","sample_embed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBiCRHhTcana","executionInfo":{"status":"ok","timestamp":1662579800069,"user_tz":240,"elapsed":135,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"66538a9b-7f13-4973-fd1a-8a93dd3c9391"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text:\n","I ruin everything ????      \n","\n","Embedded version:\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n","array([[[ 0.03494309,  0.02677599, -0.01469069, ..., -0.00333368,\n","         -0.02596296, -0.02523291],\n","        [-0.01483329,  0.00738032,  0.03382453, ...,  0.00938042,\n","         -0.04545152, -0.01981651],\n","        [-0.01544785, -0.01389501, -0.03857826, ...,  0.02884817,\n","          0.02971258, -0.02536064],\n","        ...,\n","        [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,\n","          0.00912381, -0.00024097],\n","        [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,\n","          0.00912381, -0.00024097],\n","        [ 0.01645621, -0.00589932, -0.01471175, ..., -0.02511839,\n","          0.00912381, -0.00024097]]], dtype=float32)>"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["Each token in the sentence gets turned into a length 128 feature vector."],"metadata":{"id":"NCIpbtY4g82E"}},{"cell_type":"code","source":["sample_embed[0][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zC5bOAvqcaiJ","executionInfo":{"status":"ok","timestamp":1662580395869,"user_tz":240,"elapsed":143,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}},"outputId":"21ade3d9-1477-4164-d53b-2a0cc163da51"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(128,), dtype=float32, numpy=\n","array([ 0.03494309,  0.02677599, -0.01469069, -0.03960972, -0.04943942,\n","        0.00225098,  0.02004263,  0.02774969,  0.01812977, -0.01492742,\n","       -0.02363073,  0.02237174, -0.01739722,  0.03385769, -0.00137051,\n","       -0.03835257, -0.03540713,  0.04682234,  0.01670598, -0.03170057,\n","       -0.00974751,  0.03712713,  0.03457829, -0.01487275, -0.02723646,\n","        0.00347564,  0.04279243,  0.00224274, -0.01035928,  0.03625843,\n","       -0.02449458,  0.01910372,  0.03020908,  0.02011254, -0.02584978,\n","       -0.03105422,  0.01193138, -0.03117254,  0.03420791,  0.01236663,\n","       -0.03191887, -0.00754748,  0.04793965,  0.00677263, -0.01438196,\n","       -0.0405543 , -0.03540324, -0.02837184, -0.00264017, -0.00964488,\n","       -0.03060619,  0.000412  , -0.04125547, -0.04446056, -0.00039704,\n","       -0.01311149,  0.01101058, -0.03401797, -0.03308912,  0.00245623,\n","       -0.03600726,  0.00894777, -0.01135311, -0.00068628, -0.03915939,\n","        0.00715866, -0.02089132,  0.01875928, -0.00241742, -0.03884827,\n","        0.01694694, -0.03959275,  0.00525427, -0.01971489,  0.02971849,\n","        0.02027233, -0.04756204,  0.02739265, -0.00011031, -0.02758102,\n","        0.01859525, -0.03606385,  0.00682443,  0.03694339, -0.01404322,\n","       -0.01716423,  0.01815614,  0.02758993,  0.00784829,  0.01153872,\n","        0.02624846, -0.02616305, -0.03398538, -0.0138147 , -0.03493296,\n","        0.03347934, -0.01649586, -0.04699382, -0.02422488, -0.03774772,\n","        0.03503204,  0.01018421, -0.01599883, -0.03824991, -0.02159299,\n","       -0.00236406,  0.04431834,  0.01438469, -0.0053682 ,  0.00964373,\n","        0.02028387, -0.01811736,  0.04547787, -0.01170764, -0.04787271,\n","        0.00445354, -0.01422145,  0.02417746,  0.04435888, -0.00315306,\n","       -0.03241171,  0.02417919,  0.03937734,  0.02116409,  0.0055216 ,\n","       -0.00333368, -0.02596296, -0.02523291], dtype=float32)>"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["These values might not mean much to us but they're what our computer sees each word as. When our model looks for patterns in different samples, these values will be updated as necessary."],"metadata":{"id":"0m4M8sKnhFSS"}},{"cell_type":"markdown","source":["## Modelling a text dataset"],"metadata":{"id":"IRr1_PpchNGN"}},{"cell_type":"markdown","source":["### Naive Bayes (baseline)"],"metadata":{"id":"6IEs39YYhXsb"}},{"cell_type":"markdown","source":["### Feed-forward neural network (dense model)"],"metadata":{"id":"efhoGAc4hXo8"}},{"cell_type":"markdown","source":["### Long short-term memory cells (LSTMs) model\n"],"metadata":{"id":"39DQmqruhXli"}},{"cell_type":"markdown","source":["### Gated Recurrent Units (GRU) model\n"],"metadata":{"id":"91M--Y6AhXg6"}},{"cell_type":"markdown","source":["### Bidirectional-LSTM model\n"],"metadata":{"id":"Q0mHPZQLhXZq"}},{"cell_type":"markdown","source":["### 1D Convolutional Neural Network"],"metadata":{"id":"9HrDi1WwhXLr"}},{"cell_type":"markdown","source":["\n","### TensorFlow Hub Pretrained Feature Extractor"],"metadata":{"id":"7Bb5YF3QhTnq"}},{"cell_type":"markdown","source":["\n","### Same as model 6 with 10% of training data"],"metadata":{"id":"mBOjvIl_hpfy"}},{"cell_type":"code","source":[],"metadata":{"id":"GO3ooZA6cafh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-QPaf6yicadJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CVp7BsUocaa3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"szUU0HEVcaX5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eS53xNcccaVk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DYpB0u5GcaSh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WFsfwt79caQE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KPa4cehzcaNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r6Hw5i60caKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VgQEt8JD3grr","executionInfo":{"status":"ok","timestamp":1662578450563,"user_tz":240,"elapsed":118,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SxFSDo053go7","executionInfo":{"status":"ok","timestamp":1662578450564,"user_tz":240,"elapsed":3,"user":{"displayName":"Armin Norouzi Yengeje","userId":"07700221665224727665"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## References: \n","\n","* [TensorFlow Transfer Learning Guide](https://www.tensorflow.org/tutorials/images/transfer_learning)\n","* [Transfer Learning with TensorFlow Hub tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n","* [fine-tuning a TensorFlow Hub model tutorial](https://www.tensorflow.org/hub/tf2_saved_model#fine-tuning) \n","* [experiment tracking with Weights & Biases](https://www.wandb.com/experiment-tracking)"],"metadata":{"id":"S03Aq8gplF23"}}]}