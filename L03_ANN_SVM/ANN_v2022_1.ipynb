{"cells":[{"cell_type":"markdown","metadata":{"id":"eeEgupOPc_-U"},"source":["# L02- Machine Learning Basics\n","\n","- This notebook was prepared for MECE 610: Machine Learning Control in Engineering Application course at the University of Alberta\n","- The notebook and examples are developed by **Armin Norouzi**\n","\n","**Table of Contents:**\n","\n","1.  Neural Networks (NN)\n","    1.  Forward Propagation\n","    2.  Back Propagation\n","    3.  Vectorizing NN\n","    4.  Regularization\n","    5.  NN for Regression\n","2.  Example\n","    1.  Classification based on equations\n","    2.  Classification using sklearn\n","    3.  Regression using sklearn\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gj3OJnBhc_-q"},"source":["## Neural Networks (NN)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U1lhwu1nc_-t"},"source":["### Artificial Neural Network (ANN)- Background\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_ujGvr1kc_-w"},"source":["-   is a set of algorithms that try to distinguish the correlation between a set of data using rules thought to mimic human brain operation\n","-   An ANN includes simulated neurons where each node connects to other nodes in neurons through connections that match biological axon-synapse-dendrite joints.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pD5Uf1m-c_-y"},"source":["**ANN structure**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"h0tgE0agc_-1"},"source":["-   Each link has a weight, which manages the strength of one node's influence on another.\n","-   The neurons are usually organized into multiple layers.\n","-   The nodes that receive external data as input are the input layer;\n","    -   the output layer produces the predicted output data.\n","    -   In the middle, hidden layers exist between the input layer and output layer that can be varied in size and structure.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CCSRPUoxc_-5"},"source":["### Computing a Neural Network's Output - Forward Propagation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xhMeNFkhc_-7"},"source":["-   To drive output of neural network, look at single neuron.\n","\n","-   first part of neuron in summation part which weights $w$ multiply to input $x$ and bias value $b$ is added to generate $z$\n","-   $x$ represents the input vector and contains $x_1, \\dots, x_3$\n","-   In each neuron, there is two steps of computations\n","    -   first generating $z$\n","    -   and then calculating $a$ for each node ($a$ stands for the activation function):\n","\n","\\begin{equation}\\label{eq:reg_func}\n","z = w^T x  + b\n","\\end{equation}\n","\n","-   where $x = [x_1,x_2,x_3]^T$ and $w$ is also a three component vector $ 1 \\times 3 $\n","-   then activation function $f$ is calculated based on the value of $z$ as\n","\n","$$ a = f(z)$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qeCENm5Fc_-8"},"source":["#### ANN Schematic\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C8id8kyHc_-9"},"source":["[figures/ANN_SEQ-Page-2.pdf](figures/ANN_SEQ-Page-2.pdf)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MtXR4XoVc_-9"},"source":["#### From single neuron to more complex\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TixwU0jdc_--"},"source":["-   the first layer, which is input layer, can be represent as $x = a^{[0]}$\n","-   each neuron of hidden layer can be represent as $a^{[1]}_i$ where $i$ is neuron number in hidden layer\n","-   The output of this second layer (hidden layer) can be calculated as:\n","\n","\\begin{equation}\\label{eq:nn1}\n","\\begin{split}\n","    z_1^{[1]} &= w_1^{[1]T} x + b_1^{[1]}\\\\\n","    z_2^{[1]} &= w_2^{[1]T} x + b_2^{[1]}\\\\\n","    z_3^{[1]} &= w_3^{[1]T} x + b_3^{[1]}\\\\\n","    z_4^{[1]} &= w_4^{[1]T} x + b_4^{[1]}\\\\\n","\\end{split}\n","\\end{equation}\n","\n","\\begin{equation}\\label{eq:nn2}\n","\\begin{split}\n","    a_1^{[1]} = f(z_1^{[1]}) = f(w_1^{[1]T} x + b_1^{[1]})\\\\\n","    a_2^{[1]} = f(z_2^{[1]}) = f(w_2^{[1]T} x + b_2^{[1]})\\\\\n","    a_3^{[1]} = f(z_3^{[1]}) = f(w_3^{[1]T} x + b_3^{[1]})\\\\\n","    a_4^{[1]} = f(z_4^{[1]}) = f(w_4^{[1]T} x + b_4^{[1]})\\\\\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-tQRn0Otc_-_"},"source":["#### Create Matrices of weights and bias vectors\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DnhmSfQ_c__A"},"source":["for computing these weights, we can stack weights and bias vector together to create matrix. \n","Using Eq. \\ref{eq:nn1}, we can calculate $W$ matrix:\n","\n","\\begin{equation}\\label{eq:w}\n","W^{[1]} = \\left[\n","        \\begin{tabular}{c}\n","         $w_1^{[1]T}$\\\\\n","         $w_2^{[1]T}$\\\\\n","         $w_3^{[1]T}$\\\\\n","         $w_4^{[1]T}$\\\\\n","        \\end{tabular}\n","        \\right] = \\left[\n","        \\begin{tabular}{c c c}\n","         $w_{11}^{[1]}$ & $w_{12}^{[1]}$ & $w_{13}^{[1]}$\\\\\n","         $w_{21}^{[1]}$ & $w_{22}^{[1]}$ & $w_{23}^{[1]}$\\\\\n","         $w_{31}^{[1]}$ & $w_{32}^{[1]}$ & $w_{33}^{[1]}$\\\\\n","         $w_{41}^{[1]}$ & $w_{42}^{[1]}$ & $w_{43}^{[1]}$\\\\\n","        \\end{tabular}\n","        \\right]\n","\\end{equation}\n","\n","-   and the $b$ vector\n","\n","\\begin{equation}\\label{eq:bv}\n","b^{[1]} = \\left[\n","        \\begin{tabular}{c}\n","         $b_1^{[1]T}$\\\\\n","         $b_2^{[1]T}$\\\\\n","         $b_3^{[1]T}$\\\\\n","         $b_4^{[1]T}$\\\\\n","        \\end{tabular}\n","        \\right]\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7a_kdNjxc__B"},"source":["#### Based on these two matrix, \\(z^{[1]}\\) and \\(a^{[1]}\\) are calculated.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9NQpNpiYc__B"},"source":["\\begin{equation}\\label{eq:Z}\n","Z^{[1]} = W^{[1]}x + b^{[1]} = \\left[\n","        \\begin{tabular}{c}\n","         $w_1^{[1]T}x + b_1^{[1]}$ \\\\\n","         $w_2^{[1]T}x + b_1^{[1]}$\\\\\n","         $w_3^{[1]T}x + b_1^{[1]}$\\\\\n","         $w_4^{[1]T}x + b_1^{[1]}$\\\\\n","        \\end{tabular}\n","        \\right]\n","\\end{equation}\n","\n","Finally, activation of second layer can be calculated by\n","\n","\\begin{equation}\\label{eq:a}\n","a^{[1]} = f(Z^{[1]}) = f(W^{[1]}x + b^{[1]})\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MsB1GPtZc__C"},"source":["#### Output Layer\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sKVZ1rYTc__D"},"source":["-   do the same things for output layer.\n","-   In general, for given input data $x$ as $x = [x_1,x_2,x_3]^T = a^{[0]}$,\n","\n","\\begin{equation}\\label{eq:onehidden}\n","\\begin{split}\n","    z^{[1]} &= W^{[1]} a^{[0]} + b^{[1]}\\\\\n","    a^{[1]} &= f(z^{[1]})\\\\\n","    z^{[2]} &= W^{[2]} a^{[1]} + b^{[2]}\\\\\n","    a^{[2]} &= f(z^{[2]})\\\\\n","    \\hat{y} &= a^{[2]}\\\\\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"agMSWXHlc__D"},"source":["#### Table of ANN Variable Dimensions - for example given\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YTjtno1sc__E"},"source":["#### Vectorizing across multiple examples\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tLNHWS9Ac__E"},"source":["-   Without vectorizing we have to loop through\n","-   the output can be calculated for data set with $m$ data as:\n","\n","\\begin{equation}\\label{eq:for1}\n","\\begin{split}\n","    \\text{for }i&= 1\\text{ to }m\\\\\n","    &z^{[1](i)} = W^{[1]} x^{(i)} + b^{[1]}\\\\\n","    &a^{[1](i)} = f(z^{[1](i)})\\\\\n","    &z^{[2](i)} = W^{[2]} a^{[1](i)} + b^{[2]}\\\\\n","    &a^{[2](i)} = f(z^{[2](i)})\\\\\n","    &\\hat{y}^{(i)} = a^{[2](i)}\\\\\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-mjqR-Ssc__E"},"source":["#### Problems with using Eq \\ref{eq:for1}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nNr_EFR8c__F"},"source":["1.  to train $W$ and $b$, run this `for` loop need for each iteration of the gradient descent\n","2.  vectorizing across multiple examples can make training process more efficient.\n","3.  Now vectorize Eq. \\ref{eq:for1}:\n","\n","\\begin{equation}\\label{eq:vecotor}\n","\\begin{split}\n","    Z^{[1]} &= W^{[1]} X + b^{[1]}\\\\\n","    A^{[1]} &= f(Z^{[1]})\\\\\n","    Z^{[2]} &= W^{[2]} A^{[1]} + b^{[2]}\\\\\n","    A^{[2]} &= f(Z^{[2]})\\\\\n","    \\hat{Y} &= A^{[2]}\\\\\n","\\end{split}\n","\\end{equation}\n","\n","-   where\n","\n","\\begin{equation}\\label{eq:wv}\n","\\begin{split}\n","X & = \\left[x^{(1)}, x^{(2)}, \\ldots x^{(m)}  \\right]_{n_x \\times m}\\\\\n","Z^{[1]} & = \\left[  z^{[1](1)}, z^{[1](2)}, \\ldots z^{[1](m)}  \\right]_{n_{hl} \\times m}\\\\\n","A^{[1]} & = \\left[  a^{[1](1)}, a^{[1](2)}, \\ldots a^{[1](m)}  \\right]_{n_{hl} \\times m}\\\\\n","\\end{split}\n","\\end{equation}\n","\n","-   for $X$ the column dimension $m$ is the number of data in the data set while the row dimension $n_x$ is the number of input features\n","-   for $A^{[1]}$ and $Z^{[1]}$ the row dimension $n_{hl}$ is  the number of neurons in $^{[1]}$ and the number of data is the column direction\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zHlP3S5Zc__F"},"source":["#### Activation function $f(z)$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zisUAahVc__G"},"source":["-   for ANN, the activation function of a node defines the output of that node given an input or set of inputs\n","-   for example, an integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input.\n","-   However, only nonlinear activation functions allow ANN to give complex outputs using only a small number of nodes\n","-   these activation functions are called nonlinearities and common examples are given below\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U6W7_Yb7c__G"},"source":["#### Nonlinear activation functions\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bbb5wye_c__G"},"source":["-   Sigmoid activation functions\n","\n","\\begin{equation}\\label{eq:sig}\n","\\begin{split}\n","\\sigma(z) & = \\frac{1}{1 + e^{-z}} \\\\\n","\\frac{d}{dz} \\sigma(z) & = \\sigma(z) (1 - \\sigma(z))\n","\\end{split}\n","\\end{equation}\n","\n","-   Hyperbolic tangent\n","\n","\\begin{equation}\\label{eq:tan}\n","\\begin{split}\n","\\tanh(z) & = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\\\\n","\\frac{d}{dz} \\tanh(z) & = 1 - (\\tanh(z))^2\n","\\end{split}\n","\\end{equation}\n","\n","-   Rectified linear unit (ReLU)\n","\n","\\begin{equation}\\label{eq:relu}\n","\\begin{split}\n","\\text{ReLU}(z) &= \\begin{cases}\n","         0 & \\text{if } z\\leq0 \\\\\n","         z & \\text{if } z>0 \\\\\n","    \\end{cases}\\\\\n","\\frac{d}{dz} \\text{ReLU}(z) &= \\begin{cases}\n","         0 & \\text{if } z<0 \\\\\n","         1 & \\text{if } z>0 \\\\\n","    \\end{cases}\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"phXfWSlnc__H"},"source":["#### Gradient Decent (GD) for Neural Network\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b5duRTOyc__H"},"source":["-   the cost function is defined as\n","\n","\\begin{equation}\\label{eq:cost}\n","J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m} \\sum_{i = 1}^m {L}(\\hat{Y}, Y)\n","\\end{equation}\n","\n","-   where ${L}(\\hat{y}, y)$ is the loss function\n","    -   **regression:** usually MSE\n","    -   **classification:** usually logistic\n","-   the GD can be defined as\n","\n","\\begin{equation}\\label{eq:for}\n","\\begin{split}\n","    \\text{loop \\{ }&\\\\\n","    & \\text{Compute predictions} (\\hat{y}^{(1)}, \\ldots, \\hat{y}^{(m)})\\\\\n","        & \\\\\n","    & dW^{[1]} = \\frac{dJ}{dW^{[1]}} \\\\\n","    & dW^{[2]} = \\frac{dJ}{dW^{[2]}} \\\\\n","    & db^{[1]} = \\frac{dJ}{db^{[1]}} \\\\\n","    & db^{[2]} = \\frac{dJ}{db^{[2]}} \\\\\n","    & \\\\\n","    & W^{[1]}:=W^{[1]} - \\alpha dW^{[1]} \\\\\n","    & b^{[1]}:=b^{[1]} - \\alpha db^{[1]} \\\\\n","    & W^{[2]}:=W^{[2]} - \\alpha dW^{[2]} \\\\\n","    & b^{[2]}:=b^{[2]} - \\alpha db^{[2]} \\\\ \\}\n","\\end{split}\n","\\end{equation}\n","\n","-   where $\\alpha$ is the learning rate.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bSUNwMcQc__I"},"source":["#### Backpropagation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rZ54aM3Yc__I"},"source":["-   start with simplest possible network a single neuron shown below\n","\n","[figures/ANN_SEQ-Page-2.pdf](figures/ANN_SEQ-Page-2.pdf)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l584WeCTc__J"},"source":["#### Single neuron calculations\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z4KALlcDc__J"},"source":["-   this network input vector has three element as $x = [x_1,x_2,x_3]^T$\n","-   here we derive the equation for single data set\n","-   later we need to run these equation in a `for` loop for all equation\n","-   here $m = 1$ and cost function equals to loss function as $J(w, b) = {L}(\\hat{Y}, y)$\n","-   and as $\\hat{y} = a$, the loss function is $J(w, b) = {L}(a, y)$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ve59sVWBc__J"},"source":["#### calculated derivative of cost function with respect to \\(w\\)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CBdpvtHlc__J"},"source":["-   to find $dw$ use: $dw = \\frac{\\partial {L}(a, y)}{ \\partial w}$\n","-   using the chain rule this can be written as: $dw = \\frac{\\partial {L}(a, y)}{ \\partial a} \\frac{\\partial a}{ \\partial z} \\frac{\\partial z}{ \\partial w}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iFPnE9eoc__K"},"source":["#### Define \\(da\\) and \\(dz\\)  from Loss function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0L0jPthIc__K"},"source":["$$da = \\frac{\\partial {L}(a, y)}{ \\partial a} $$\n","\n","\\begin{equation} \\label{dz}\n","dz = \\frac{\\partial {L}(a, y)}{ \\partial a} \\frac{\\partial a}{ \\partial z}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y-Wp2hG8c__K"},"source":["#### Logistic loss function for \\(a\\) to get \\(da\\)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vcspZx95c__L"},"source":["-   the logistic loss function is $L = -y \\log(a) - (1-y) \\log(1-a) $\n","-   first calculate $da$ by using logistic loss function\n","\n","\\begin{equation} \\label{da}\n","\\begin{split}\n","da &= \\frac{\\partial}{ \\partial a} (-y \\log(a) - (1-y) \\log(1-a))\\\\\n","& = \\frac{-y}{a} + \\frac{1-y}{1-a}\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N48c3N-Zc__L"},"source":["#### Sigmoid function to get \\(dz\\)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9ntlhTeTc__L"},"source":["-   using sigmoid function, $a = \\sigma(z)$ and Eq. \\ref{dz} to get $dz$ as\n","\n","\\begin{equation}\\label{dz2}\n","\\begin{split}\n","dz &= \\frac{\\partial {L}(a, y)}{ \\partial a} \\frac{\\partial a}{ \\partial z}\\\\\n","&= da \\frac{\\partial a}{ \\partial z}\\\\\n","&=da (a)(1-a)\n","\\end{split}\n","\\end{equation}\n","\n","-   substituting Eq.\\ref{da} in Eq.\\ref{dz2}, $dz$ is simplified to\n","\n","\\begin{equation}\\label{dz3}\n","\\begin{split}\n","dz&=\\frac{-y}{a} + \\frac{1-y}{1-a} (a)(1-a) \\\\\n","&= a - y\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C6nNvHPsc__L"},"source":["#### Find \\(dw\\)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_jbeqNtkc__M"},"source":["-   next substituting $\\partial z / \\partial w = x$, $dw$ can be found as\n","\n","\\begin{equation}\\label{dw}\n","\\begin{split}\n","dz&= a - y\\\\\n","dw &= x dz\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5BVGRPf3c__M"},"source":["#### Find \\(dw\\)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mWRCIzmAc__M"},"source":["-   repeating the same procedure for $db$ which is\n","\n","\\begin{equation}\\label{db}\n","\\begin{split}\n","dz&= a - y\\\\\n","db &= dz\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"obuQ6JZZc__N"},"source":["#### To generalizing these equations\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vHV3X-NFc__N"},"source":["-   now add a hidden layer\n","-   the difference is we need to go through this process one more time using the same concepts\n","-   here the hidden layer activation function is a general form of $f(z)$, so $f'(z) = \\partial f / \\partial z$\n","\n","\\begin{equation}\\label{derivates}\n","\\begin{split}\n","dz^{[2]} &= a^{[2]} - y\\\\\n","dW^{[2]} &= dz^{[2]}a^{[2]T}\\\\\n","db^{[2]} &= dz^{[2]}\\\\\n","dz^{[1]} &= W^{[2]}dz^{[2]} \\odot f'^{[1]}(z^{[1]})\\\\\n","dw^{[1]} &= dz^{[1]}x^T\\\\\n","db^{[1]} &= dz^{[1]}\\\\\n","\\end{split}\n","\\end{equation}\n","\n","-   where \\`\\`$\\odot$'' is element wise multiplication\n","-   as this must be run for the entire data set it is an inefficient way to run GD\n","-   rather, vectorizing by stacking different examples in a vector as:\n","\n","\\begin{equation}\\label{derivatesVector}\n","\\begin{split}\n","dZ^{[2]} &= A^{[2]} - Y\\\\\n","dW^{[2]} &= \\frac{1}{m} dZ^{[2]}A^{[2]T}\\\\\n","db^{[2]} &= \\frac{1}{m} \\sum_{i=1}^m  dZ^{[2]}\\\\\n","dZ^{[1]} &= W^{[2]T}dZ^{[2]} \\odot f'^{[1]}(Z^{[1]})\\\\\n","dW^{[1]} &= \\frac{1}{m} dZ^{[1]}X^T\\\\\n","db^{[1]} &= \\frac{1}{m} \\sum_{i=1}^m  dZ^{[1]}\\\\\n","\\end{split}\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ObzXe92kc__N"},"source":["#### Regularization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cimyuaq4c__N"},"source":["-   the Cost function can be rewritten by adding regularization term as\n","\n","\\begin{equation}\\label{eq:costreg}\n","\tJ(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m} \\sum_{i = 1}^m {L}(\\hat{Y}, Y) + \\frac{\\lambda}{2m} \\sum_{l = 1}^L ||w^{[l]}||_2^2\n","\\end{equation}\n","\n","-   where ${L}(\\hat{y}, y)$ is loss function\n","-   $\\lambda$ is regularization coefficient\n","-   and $||w^{[l]}||_2^2$ is the Euclidean norm defined as\n","\n","\\begin{equation}\\label{eq:costreg}\n","\t||w^{[l]}||_2^2 = \\sum_{i = 1}^{n^{[l]}} \\sum_{j = 1}^{n^{[l-1]}} (w_{ij}^{[l]})^2\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IggPwRq6c__O"},"source":["#### Gradient decent with regularization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P9r4pXMyc__O"},"source":["-   the derivatives for GD are now:\n","\n","\\begin{equation}\n","\tdW^{[l]} = \\frac{dJ}{dW^{[l]}} = dW_{\\text{without regularization}}^{[l]} + \\frac{\\lambda}{m} W^{[l]}\n","\\end{equation}\n","\n","-   where $dW_{\\text{without regularization}}^{[l]} $ is from Eq \\ref{derivatesVector}\n","-   the weights update can be written as\n","\n","\\begin{equation}\n","\tW^{[l]} = W^{[l]} - \\alpha ( dW_{\\text{without regularization}}^{[l]} + \\frac{\\lambda}{m} W^{[l]})\n","\\end{equation}\n","\n","or rewritten in this form\n","\n","\\begin{equation}\n","\tW^{[l]} = ( 1- \\frac{\\alpha \\lambda}{m})  W^{[l]} - \\alpha ( dW_{\\text{without regularization}}^{[l]})\n","\\end{equation}\n","\n","-   as $ ( 1- \\frac{\\alpha \\lambda}{m}) $ is always less than one\n","-   in this formulation the weights decrease for each iteration update\n","-   This is the one reason, L2 regularization also called weight decay.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"crCCQO1Xc__P"},"source":["#### Regression problem\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qBesz4Kqc__P"},"source":["-   Regression models deals with predicting a continuous value\n","-   for example given injected fuel amount, common rail pressure, and load, predict the NOx emission of a diesel engine\n","-   The mathematics behind regression follows that of classification but with a different loss function\n","    -   i.e. instead of logistic loss function the MSE loss function is used for the regression problem\n","        -   Alternative to MSE is the MAE loss\n","    -   a \\`\\`Regression Loss Function'' is used.\n","-   Some different types of regression loss function for regression used for different purposes are\n","    -   Mean Squared Error Loss\n","    -   Mean Squared Logarithmic Error Loss\n","    -   Mean Absolute Error Loss\n","-   in this course, the python `sklearn` library is used to illustrate different examples\n","-   this library uses the most well-known loss function the Mean Squared Error Loss which is\n","\n","\\begin{equation}\n","\t{L}(\\hat{y}, y) = \\frac{1}{m} \\sum_{i = 1}^m (\\hat{y}_i - y_i)^2\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UoaCeh0wc__Q"},"source":["### ANN - Example classification - based on equations\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K9z5OVFuc__Q"},"source":["#### One Hidden layer ANN - Solving Gradient descent in ANN\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"8zMqLly4c__Q","executionInfo":{"status":"ok","timestamp":1647396646377,"user_tz":360,"elapsed":1705,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["#import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import math\n","import operator\n","import matplotlib.pyplot as plt\n","get_ipython().run_line_magic('matplotlib', 'inline')\n","\n","\n","# from testCases_v2 import *\n","import sklearn\n","import sklearn.datasets\n","import sklearn.linear_model"]},{"cell_type":"markdown","metadata":{"id":"6SEymIhyc__S"},"source":["#### Import data and show data and plot\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"enr51SLec__S","outputId":"8fd2416d-6f92-4163-b8b0-d1d87c288490","colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"status":"error","timestamp":1647396647428,"user_tz":360,"elapsed":1054,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9514b3fe8e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Engine_NOx_classification.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Engine_NOx_classification.csv'"]}],"source":["data = pd.read_csv('Engine_NOx_classification.csv')\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwqRcRgkc__U","executionInfo":{"status":"aborted","timestamp":1647396646843,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["cdf = data[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'High NOx']]\n","\n","msk = np.random.rand(len(data)) < 0.8\n","train = cdf[msk]\n","test = cdf[~msk]"]},{"cell_type":"markdown","metadata":{"id":"FNUv57kbc__U"},"source":["#### Plot training data then test data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xI6_A_sec__U","executionInfo":{"status":"aborted","timestamp":1647396646844,"user_tz":360,"elapsed":26,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["colors = {0: 'blue', 1:'red', 2:'green', 3:'coral', 4:'orange', 5:'black'}\n","\n","area = 300\n","# area = 200\n","plt.figure(1, figsize=(25, 6))\n","plt.scatter(train['Load [ft.lb]'], train['Engine speed [rpm]'], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"AylQWt1nc__V"},"source":["##### Plot test data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJOhENU_c__V","executionInfo":{"status":"aborted","timestamp":1647396646845,"user_tz":360,"elapsed":26,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["area = 300\n","# area = 200\n","plt.figure(1, figsize=(25, 6))\n","plt.scatter(test['Load [ft.lb]'], test['Engine speed [rpm]'], s=area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)\n","\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"IqNmUZJlc__W"},"source":["#### Set Test and training data - preprocessing\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIbj8btKc__W","executionInfo":{"status":"aborted","timestamp":1647396646846,"user_tz":360,"elapsed":26,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import preprocessing\n","\n","# train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","# train_y = np.asanyarray(train[['High NOx']])\n","\n","# test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","# test_y = np.asanyarray(test[['High NOx']])\n","\n","train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]']])\n","train_y = np.asanyarray(train[['High NOx']])\n","\n","test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]']])\n","test_y = np.asanyarray(test[['High NOx']])\n","\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)"]},{"cell_type":"markdown","metadata":{"id":"8dD5fiLPc__W"},"source":["#### Assign train and test data to the correct size\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7gSK73xc__X","executionInfo":{"status":"aborted","timestamp":1647396646846,"user_tz":360,"elapsed":26,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["X = X_train_minmax.T\n","Y = train_y.T\n","\n","shape_X = X.shape\n","shape_Y = Y.shape\n","m = shape_X[1]\n","\n","print ('The shape of X is: ' + str(shape_X))\n","print ('The shape of Y is: ' + str(shape_Y))\n","print ('I have m = %d training examples!' % (m))"]},{"cell_type":"markdown","metadata":{"id":"nleJX-e3c__X"},"source":["#### Neural Network model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xt2qFKJcc__X"},"source":["-   now train a Neural Network with a single hidden layer.\n","-   the Math Equations are for one example $x^{(i)}$:\n","\n","$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}$$ \n","$$a^{[1] (i)} = \\tanh(z^{[1] (i)})$$\n","$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}$$\n","$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})$$\n","$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}$$\n","\n","-   given the predictions on all the examples, you can also compute the cost $J$ as follows:\n","\n","$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small $$\n","\n","-   Reminder: The general methodology to build a Neural Network is to:\n","    1.  Define the neural network structure ( # of input units,  # of hidden units, etc).\n","    2.  Initialize the model's parameters\n","    3.  Loop:\n","        -   Implement forward propagation\n","        -   Compute loss\n","        -   Implement backward propagation to get the gradients\n","        -   Update parameters (gradient descent)\n","-   often build helper functions to compute steps 1-3\n","-   then merge them into one function we call `nn_model()`\n","-   Once you've built `nn_model()` and learn the right parameters\n","-   now can make predictions on new data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Bnf3w9T4c__Y"},"source":["#### 1) Neural Network structure - define the helper functions\n","\n"]},{"cell_type":"markdown","metadata":{"id":"08BVozWsc__Y"},"source":["-   Define three variables:\n","    -   `n_x`: the size of the input layer\n","    -   `n_h`: the size of the hidden layer (set this to 4)\n","    -   `n_y`: the size of the output layer\n","-   Hint:  Use shapes of X and Y to find `n_x` and `n_y`.\n","-   Also, hard code the hidden layer size to be 4.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9EHn-Gcc__Y","executionInfo":{"status":"aborted","timestamp":1647396646847,"user_tz":360,"elapsed":26,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def layer_sizes(X, Y):\n","    \"\"\"\n","    Arguments:\n","    X -- input dataset of shape (input size, number of examples)\n","    Y -- labels of shape (output size, number of examples)\n","    \n","    Returns:\n","    n_x -- the size of the input layer\n","    n_h -- the size of the hidden layer\n","    n_y -- the size of the output layer\n","    \"\"\"\n","    \n","    n_x = X.shape[0] # size of input layer\n","    n_h = 4\n","    n_y = Y.shape[0] # size of output layer\n","\n","    return (n_x, n_h, n_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKUpEx9xc__Z","executionInfo":{"status":"aborted","timestamp":1647396646847,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["(n_x, n_h, n_y) = layer_sizes(X, Y)\n","print(\"The size of the input layer is: n_x = \" + str(n_x))\n","print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n","print(\"The size of the output layer is: n_y = \" + str(n_y))"]},{"cell_type":"markdown","metadata":{"id":"BPj4-hAjc__Z"},"source":["#### 2) Initialize the model parameters\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WrWYjIwcc__Z"},"source":["-   goal: Implement the function `initialize_parameters()`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OccX9j0gc__a","executionInfo":{"status":"aborted","timestamp":1647396646847,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def initialize_parameters(n_x, n_h, n_y):\n","    \"\"\"\n","    Argument:\n","    n_x -- size of the input layer\n","    n_h -- size of the hidden layer\n","    n_y -- size of the output layer\n","    \n","    Returns:\n","    params -- python dictionary containing your parameters:\n","                    W1 -- weight matrix of shape (n_h, n_x)\n","                    b1 -- bias vector of shape (n_h, 1)\n","                    W2 -- weight matrix of shape (n_y, n_h)\n","                    b2 -- bias vector of shape (n_y, 1)\n","    \"\"\"\n","    \n","    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n","    \n","    # generate random weight and zero bias\n","    W1 = np.random.randn(n_h, n_x) * 0.01\n","    b1 = np.zeros((n_h, 1))\n","    W2 = np.random.randn(n_y, n_h) * 0.01\n","    b2 = np.zeros((n_y, 1))\n","\n","    \n","    assert (W1.shape == (n_h, n_x))\n","    assert (b1.shape == (n_h, 1))\n","    assert (W2.shape == (n_y, n_h))\n","    assert (b2.shape == (n_y, 1))\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yet5s6Mpc__a","executionInfo":{"status":"aborted","timestamp":1647396646848,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["parameters = initialize_parameters(n_x, n_h, n_y)\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"wWDhCaJrc__b"},"source":["#### 3) The Loop\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qf8bVIvsc__b"},"source":["Implement `forward_propagation()`.\n","\n","-   Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n","-   Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n","-   Values needed in the backpropagation are stored in `cache`. The `cache` will be given as an input to the backpropagation function.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Eh6zMrIPc__b"},"source":["#### The sigmoid function\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dVaOHphDc__c","executionInfo":{"status":"aborted","timestamp":1647396646848,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def sigmoid(x):\n","    \"\"\"\n","    Compute the sigmoid of x\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    s = 1/(1+np.exp(-x))\n","    return s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17ELWfUtc__c","executionInfo":{"status":"aborted","timestamp":1647396646849,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def forward_propagation(X, parameters):\n","    \"\"\"\n","    Argument:\n","    X -- input data of size (n_x, m)\n","    parameters -- python dictionary containing your parameters (output of initialization function)\n","    \n","    Returns:\n","    A2 -- The sigmoid output of the second activation\n","    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n","    \"\"\"\n","    # Retrieve each parameter from the dictionary \"parameters\"\n","\n","    \n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","\n","    \n","    \n","    # Implement Forward Propagation to calculate A2 (probabilities)\n","\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = np.tanh(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = sigmoid(Z2)\n","\n","    \n","    assert(A2.shape == (1, X.shape[1]))\n","    \n","    cache = {\"Z1\": Z1,\n","             \"A1\": A1,\n","             \"Z2\": Z2,\n","             \"A2\": A2}\n","    \n","    return A2, cache"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2fGHOcRc__c","executionInfo":{"status":"aborted","timestamp":1647396646849,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["A2, cache = forward_propagation(X, parameters)\n","print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"]},{"cell_type":"markdown","metadata":{"id":"Kb6-dZ9Lc__d"},"source":["#### 4) Compute the cost function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DF7S-A9yc__d"},"source":["$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g0YaVi8lc__d"},"source":["#### Compute the cost function - python function\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2MgTdVDc__e","executionInfo":{"status":"aborted","timestamp":1647396646850,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def compute_cost(A2, Y, parameters):\n","    \"\"\"\n","    Computes the cross-entropy cost given in equation (13)\n","    \n","    Arguments:\n","    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n","    Y -- \"true\" labels vector of shape (1, number of examples)\n","    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n","    [Note that the parameters argument is not used in this function, \n","    but the auto-grader currently expects this parameter.\n","    Future version of this notebook will fix both the notebook \n","    and the auto-grader so that `parameters` is not needed.\n","    For now, please include `parameters` in the function signature,\n","    and also when invoking this function.]\n","    \n","    Returns:\n","    cost -- cross-entropy cost given equation (13)\n","    \n","    \"\"\"\n","    \n","    m = Y.shape[1] # number of example\n","\n","    # Compute the cross-entropy cost\n","\n","    logprobs = Y*np.log(A2) + (1-Y)*np.log(1-A2)\n","    cost = (-1/m) * np.sum(logprobs, axis=1)\n","\n","    \n","    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n","                                    # E.g., turns [[17]] into 17 \n","    assert(isinstance(cost, float))\n","    \n","    return cost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrZXvduqc__e","executionInfo":{"status":"aborted","timestamp":1647396646850,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["print(\"cost = \" + str(compute_cost(A2, Y, parameters)))"]},{"cell_type":"markdown","metadata":{"id":"pdLkbWgac__e"},"source":["#### 5) Backward propagation.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R-j-5v29c__f"},"source":["-   build a vectorized implementation.\n","\n","$$ \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$$ \n","$$ \\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $$\n","$$ \\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$$\n","$$ \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } \\odot ( 1 - a^{[1] (i) 2}) $$\n","$$ \\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $$\n","$$ \\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$$\n","\n","-   Note that $\\odot$ denotes element wise multiplication.\n","-   A common notation in machine learning coding is:\n","    -   dW1 = $ \\frac{\\partial \\mathcal{J} }{ \\partial W_1 } $\n","    -   db1 = $ \\frac{\\partial \\mathcal{J} }{ \\partial b_1 } $\n","    -   dW2 = $ \\frac{\\partial \\mathcal{J} }{ \\partial W_2 } $\n","    -   db2 = $ \\frac{\\partial \\mathcal{J} }{ \\partial b_2 } $\n","    -   dZ1 = $g^{[1]'}Z^{[1]}$\n","    -   dZ2 = $a^{[2]} - y $\n","-   Tips:\n","    -   To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$.\n","    -   Since $g^{[1]}(.)$ is the `tanh` activation function,\n","    -   if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$.\n","    -   So you can compute $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UjBw28Eec__f"},"source":["#### Back propagation function and compute gradients\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DEK7xuoc__f","executionInfo":{"status":"aborted","timestamp":1647396646851,"user_tz":360,"elapsed":25,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def backward_propagation(parameters, cache, X, Y):\n","    \"\"\"\n","    Implement the backward propagation using the instructions above.\n","    \n","    Arguments:\n","    parameters -- python dictionary containing our parameters \n","    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n","    X -- input data of shape (2, number of examples)\n","    Y -- \"true\" labels vector of shape (1, number of examples)\n","    \n","    Returns:\n","    grads -- python dictionary containing your gradients with respect to different parameters\n","    \"\"\"\n","    m = X.shape[1]\n","    \n","    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n","\n","    W1 = parameters[\"W1\"]\n","    W2 = parameters[\"W2\"]\n","\n","        \n","    # Retrieve also A1 and A2 from dictionary \"cache\".\n","\n","    A1 = cache[\"A1\"]\n","    A2 = cache[\"A2\"]\n","\n","    \n","    # Backward propagation: calculate dW1, db1, dW2, db2. \n","\n","    dZ2 = A2 - Y\n","    dW2 = (1/m) * np.dot(dZ2, A1.T)\n","    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n","    dZ1 = np.dot(W2.T, dZ2) * (1-np.power(A1, 2))\n","    dW1 = (1/m) * np.dot(dZ1, X.T)\n","    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n","\n","    \n","    grads = {\"dW1\": dW1,\n","             \"db1\": db1,\n","             \"dW2\": dW2,\n","             \"db2\": db2}\n","    \n","    return grads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-Zi2x6zc__g","executionInfo":{"status":"aborted","timestamp":1647396647033,"user_tz":360,"elapsed":207,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["grads = backward_propagation(parameters, cache, X, Y)\n","print (\"dW1 = \"+ str(grads[\"dW1\"]))\n","print (\"db1 = \"+ str(grads[\"db1\"]))\n","print (\"dW2 = \"+ str(grads[\"dW2\"]))\n","print (\"db2 = \"+ str(grads[\"db2\"]))"]},{"cell_type":"markdown","metadata":{"id":"rB-95DIKc__g"},"source":["#### 6) Implement the gradient decent rule\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U0oGJsoZc__h"},"source":["-   Use gradient descent. Use `(dW1, db1, dW2, db2)`\n","-   to update `(W1, b1, W2, b2)`.\n","-   General gradient descent rule: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$\n","-   where $\\alpha$ is the learning rate and $ \\theta$ represents a parameter.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1mZlxoWpc__h"},"source":["#### Illustration the gradient descent algorithm\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nZpK79HPc__h"},"source":["-   with a good learning rate (converging) and a bad learning rate (diverging) - later.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TVLrSLRMc__i","executionInfo":{"status":"aborted","timestamp":1647396647034,"user_tz":360,"elapsed":208,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Updates parameters using the gradient descent update rule given above\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients \n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","    \"\"\"\n","    # Retrieve each parameter from the dictionary \"parameters\"\n","\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","\n","    \n","    # Retrieve each gradient from the dictionary \"grads\"\n","\n","    dW1 = grads[\"dW1\"]\n","    db1 = grads[\"db1\"]\n","    dW2 = grads[\"dW2\"]\n","    db2 = grads[\"db2\"]\n","\n","    \n","    # Update rule for each parameter\n","\n","    W1 = W1 - learning_rate * dW1\n","    b1 = b1 - learning_rate * db1\n","    W2 = W2 - learning_rate * dW2\n","    b2 = b2 - learning_rate * db2\n","\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"reeX_NA_c__i","executionInfo":{"status":"aborted","timestamp":1647396647034,"user_tz":360,"elapsed":207,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["parameters = update_parameters(parameters, grads, 0.01)\n","\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"3q6CLckQc__j"},"source":["#### Now combine 1 to 6\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3f7v4s8xc__j"},"source":["Combine 1 &#x2013; 6 in `nn_model()`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpST-AXAc__j","executionInfo":{"status":"aborted","timestamp":1647396647034,"user_tz":360,"elapsed":206,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False, learning_rate = 0.1):\n","    \"\"\"\n","    Arguments:\n","    X -- dataset of shape (2, number of examples)\n","    Y -- labels of shape (1, number of examples)\n","    n_h -- size of the hidden layer\n","    num_iterations -- Number of iterations in gradient descent loop\n","    print_cost -- if True, print the cost every 1000 iterations\n","    \n","    Returns:\n","    parameters -- parameters learnt by the model. They can then be used to predict.\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    n_x = layer_sizes(X, Y)[0]\n","    n_y = layer_sizes(X, Y)[2]\n","    \n","    # Initialize parameters\n","    parameters = initialize_parameters(n_x, n_h, n_y)\n","    \n","    # Loop (gradient descent)\n","\n","    for i in range(0, num_iterations):\n","         \n","        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n","        A2, cache = forward_propagation(X, parameters)\n","        \n","        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n","        cost = compute_cost(A2, Y, parameters)\n"," \n","        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n","        grads = backward_propagation(parameters, cache, X, Y)\n"," \n","        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","\n","        \n","        # Print the cost every 1000 iterations\n","        if print_cost and i % 1000 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","\n","    return parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgXwLv0vc__k","executionInfo":{"status":"aborted","timestamp":1647396647034,"user_tz":360,"elapsed":205,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["parameters = nn_model(X, Y, 4, num_iterations=10000, print_cost=True)\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"]},{"cell_type":"markdown","metadata":{"id":"16us5V50c__k"},"source":["#### Make Predictions - plotting\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gl5JdLYxc__k"},"source":["-   Reminder: predictions are: $y_{prediction} =  \\begin{cases}\n","           1 & \\text{if activation} > 0.5 \\\\\n","           0 & \\text{otherwise}\n","         \\end{cases}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2Hx5EpQEc__l"},"source":["##### For example,\n","\n"]},{"cell_type":"markdown","metadata":{"id":"biyqOhvQc__l"},"source":["-   if you would like to set the entries of a matrix X to 0 and 1 based on a threshold\n","-   you would set: `X_new = (X > threshold)`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DG4JsaBc__l","executionInfo":{"status":"aborted","timestamp":1647396647035,"user_tz":360,"elapsed":205,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def predict(parameters, X):\n","    \"\"\"\n","    Using the learned parameters, predicts a class for each example in X\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    X -- input data of size (n_x, m)\n","    \n","    Returns\n","    predictions -- vector of predictions \n","    \"\"\"\n","    \n","    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n","\n","    A2, cache = forward_propagation(X, parameters)\n","#     print(A2)\n","    predictions = A2 > 0.5\n","#     predictions = [1 if x > 0.5 else 0 for x in A2[0]]\n","#     print(predictions)\n","\n","    \n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YhMpA9Ec__m","executionInfo":{"status":"aborted","timestamp":1647396647035,"user_tz":360,"elapsed":205,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["predictions = predict(parameters, X)\n","print(\"predictions mean = \" + str(np.mean(predictions)))"]},{"cell_type":"markdown","metadata":{"id":"bi85h-Mfc__m"},"source":["#### Build a model with a n_h-dimensional hidden layer and print accuracy\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_MojjXcVc__m"},"source":["Can change the dimension of the hidden layer using `n_h`\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IT_PzwJGc__n","executionInfo":{"status":"aborted","timestamp":1647396647035,"user_tz":360,"elapsed":204,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True, learning_rate = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WcvxeHzqc__n","executionInfo":{"status":"aborted","timestamp":1647396647035,"user_tz":360,"elapsed":203,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["# Print accuracy - training\n","predictions = predict(parameters, X)\n","print ('Training Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JABe7aYkc__n","executionInfo":{"status":"aborted","timestamp":1647396647036,"user_tz":360,"elapsed":204,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["Xts = X_test_minmax.T\n","Yts = test_y.T\n","\n","# Print accuracy\n","predictions_test = predict(parameters, Xts)\n","print ('Testing Accuracy: %d' % float((np.dot(Yts,predictions_test.T) + np.dot(1-Yts,1-predictions_test.T))/float(Yts.size)*100) + '%')"]},{"cell_type":"markdown","metadata":{"id":"15w5RwjTc__o"},"source":["#### Now plot\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_H6k7tWtc__o","executionInfo":{"status":"aborted","timestamp":1647396647036,"user_tz":360,"elapsed":203,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["colors = {0: 'lightcoral', 1:'aqua', 2:'green', 3:'coral', 4:'orange', 5:'black'}\n","\n","area = 300\n","# area = 200\n","plt.figure(1, figsize=(25, 6))\n","plt.scatter(train['Load [ft.lb]'], train['Engine speed [rpm]'], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)\n","\n","def plot_decision_boundary(model, X, y):\n","    # Set min and max values and give it some padding\n","    x_min, x_max = X[0, :].min() - 0.1, X[0, :].max() + 0.1\n","    y_min, y_max = X[1, :].min() - 0.1, X[1, :].max() + 0.1\n","    h = 0.01\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    \n","#     x_min1, x_max1 = X[2, :].min() - 0.1, X[2, :].max() + 0.1\n","#     y_min1, y_max1 = X[3, :].min() - 0.1, X[3, :].max() + 0.1\n","#     h = 0.01\n","#     # Generate a grid of points with distance h between them\n","#     xx1, yy1 = np.meshgrid(np.arange(x_min1, x_max1, h), np.arange(y_min1, y_max1, h))\n","    \n","    \n","    # Predict the function value for the whole grid\n","#     Z = model(np.c_[xx.ravel(), yy.ravel(), xx1.ravel(), yy1.ravel()])\n","    Z = model(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    area = 30\n","    plt.scatter(X[0,:], X[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","    plt.xlabel('Load [ft.lb]', fontsize=18)\n","    plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWfTpSdVc__p","executionInfo":{"status":"aborted","timestamp":1647396647036,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n","plt.title(\"Decision Boundary for hidden layer size \" + str(4))\n","n_h"]},{"cell_type":"markdown","metadata":{"id":"SUP8u9npc__p"},"source":["#### Vary the size of the hidden layer\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9Cd7_0b4c__p"},"source":["Plot as a function of hidden layers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9YI4pv1c__q","executionInfo":{"status":"aborted","timestamp":1647396647037,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.figure(figsize=(16, 32))\n","hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n","for i, n_h in enumerate(hidden_layer_sizes):\n","    plt.subplot(5, 2, i+1)\n","    plt.title('Hidden Layer of size %d' % n_h)\n","    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n","    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n","    predictions = predict(parameters, X)\n","    predictions_test = predict(parameters, Xts)\n","    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n","    accuracy_test = float((np.dot(Yts,predictions_test.T) + np.dot(1-Yts,1-predictions_test.T))/float(Yts.size)*100)\n","    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n","    print (\"Accuracy test for {} hidden units: {} %\".format(n_h, accuracy_test))"]},{"cell_type":"markdown","metadata":{"id":"nDivN5tqc__q"},"source":["### ANN - Example L2 Regularization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"v_R5JlBNc__q"},"source":["#### Use L2 regularization to avoid overfitting\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ORgzRSXAc__r"},"source":["-   The standard way to avoid overfitting is called L2 regularization\n","-   It consists of appropriately modifying your cost function, from:\n","\n","$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} $$\n","\n","-   To:\n","\n","$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{logistic} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} $$\n","\n","-   modify the cost and observe the consequences.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LRkIhDU_c__r"},"source":["#### structure of cost with regularization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ehIXbhxac__r"},"source":["The structure to compute cost with regularization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"D3RKzVJDc__s"},"source":["##### need to have new functions for\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WIM1t-ODc__s"},"source":["1.  compute the cost function: `compute_cost_with_regularization()`\n","2.  forward propagation:  `forward_propagation_reg`\n","3.  backward propagation: `Backward_propagation_with_regularization`\n","4.  Prediction; `predict_reg`\n","5.  Model: `model`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FC8i1amsc__s"},"source":["#### 1) Regularization cost function\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwY43qSsc__t","executionInfo":{"status":"aborted","timestamp":1647396647037,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def compute_cost_with_regularization(A2, Y, parameters, lambd):\n","    \"\"\"\n","    Implement the cost function with L2 regularization. See formula (2) above.\n","    \n","    Arguments:\n","    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n","    Y -- \"true\" labels vector, of shape (output size, number of examples)\n","    parameters -- python dictionary containing parameters of the model\n","    \n","    Returns:\n","    cost - value of the regularized loss function (formula (2))\n","    \"\"\"\n","    m = Y.shape[1]\n","    W1 = parameters[\"W1\"]\n","    W2 = parameters[\"W2\"]\n","\n","    \n","    cross_entropy_cost = compute_cost(A2, Y, parameters) # This gives you the cross-entropy part of the cost\n","    \n","    #(approx 1 lines of code)\n","    # L2_regularization_cost = \n","    L2_regularization_cost = 1/m * lambd/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n","    \n","    \n","    cost = cross_entropy_cost + L2_regularization_cost\n","    \n","    return cost"]},{"cell_type":"markdown","metadata":{"id":"sQmOY6wyc__t"},"source":["#### 2) Backward propagation with regularization\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TuDaRSWYc__t"},"source":["-   implement `Backward_propagation_with_regularization`\n","-   the changes from before are only `dW1`, `dW2` and `dW3`.\n","-   for each, add the regularization term's gradient $\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQxF_pYtc__t","executionInfo":{"status":"aborted","timestamp":1647396647037,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def backward_propagation_with_regularization(X, Y, cache, lambd):\n","    \"\"\"\n","    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- \"true\" labels vector, of shape (output size, number of examples)\n","    cache -- cache output from forward_propagation()\n","    lambd -- regularization hyperparameter, scalar\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    (Z1, A1, W1, b1, Z2, A2, W2, b2) = cache\n","    \n","    \n","\n","    dZ2 =  A2 - Y\n","\n","\n","\n","    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd/m * W2\n","    \n","\n","    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n","    \n","    dA1 = np.dot(W2.T, dZ2)\n","    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","\n","\n","    dW1 = 1./m * np.dot(dZ1, X.T) + lambd/m * W1\n","    \n","\n","    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n","    \n","    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n","                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients"]},{"cell_type":"markdown","metadata":{"id":"cmXOyP1xc__u"},"source":["#### 3) Forward propagation with regularization\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdGbszovc__u","executionInfo":{"status":"aborted","timestamp":1647396647037,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def forward_propagation_reg(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation (and computes the loss) presented in Figure 2.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                    W1 -- weight matrix of shape ()\n","                    b1 -- bias vector of shape ()\n","                    W2 -- weight matrix of shape ()\n","                    b2 -- bias vector of shape ()\n","                    W3 -- weight matrix of shape ()\n","                    b3 -- bias vector of shape ()\n","    \n","    Returns:\n","    loss -- the loss function (vanilla logistic loss)\n","    \"\"\"\n","        \n","    # retrieve parameters\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","\n","    \n","    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = sigmoid(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = sigmoid(Z2)\n","\n","    \n","    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n","    \n","    return A2, cache"]},{"cell_type":"markdown","metadata":{"id":"BLzpbRQJc__v"},"source":["#### 4) Predict with regularization\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHF9kVLgc__v","executionInfo":{"status":"aborted","timestamp":1647396647038,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def predict_reg(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  n-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    p = np.zeros((1,m), dtype = np.int)\n","    \n","    # Forward propagation\n","    a2, caches = forward_propagation(X, parameters)\n","    \n","    # convert probas to 0/1 predictions\n","    for i in range(0, a2.shape[1]):\n","        if a2[0,i] > 0.5:\n","            p[0,i] = 1\n","        else:\n","            p[0,i] = 0\n","\n","    # print results\n","\n","    #print (\"predictions: \" + str(p[0,:]))\n","    #print (\"true labels: \" + str(y[0,:]))\n","    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n","    \n","    return p"]},{"cell_type":"markdown","metadata":{"id":"pdYgGcUlc__v"},"source":["#### 5) Implement the model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NF4hvFlCc__v","executionInfo":{"status":"aborted","timestamp":1647396647038,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["def model(X, Y,n_h, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0.01):\n","    \"\"\"\n","    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n","    \n","    Arguments:\n","    X -- input data, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n","    learning_rate -- learning rate of the optimization\n","    num_iterations -- number of iterations of the optimization loop\n","    print_cost -- If True, print the cost every 10000 iterations\n","    lambd -- regularization hyperparameter, scalar\n","    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n","    \n","    Returns:\n","    parameters -- parameters learned by the model. They can then be used to predict.\n","    \"\"\"\n","        \n","    grads = {}\n","    costs = []                            # to keep track of the cost\n","    m = X.shape[1]                        # number of examples\n","    n_x = layer_sizes(X, Y)[0]\n","    n_y = layer_sizes(X, Y)[2]\n","    \n","    # Initialize parameters\n","    parameters = initialize_parameters(n_x, n_h, n_y)\n","\n","\n","    # Loop (gradient descent)\n","\n","    for i in range(0, num_iterations):\n","\n","        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n","        \n","        a2, cache = forward_propagation_reg(X, parameters)\n","\n","        \n","        # Cost function\n","        if lambd == 0:\n","            cost = compute_cost(a2, Y)\n","        else:\n","            cost = compute_cost_with_regularization(a2, Y, parameters, lambd)\n","            \n","        # Backward propagation.\n","\n","        if lambd == 0:\n","            grads = backward_propagation(X, Y, cache)\n","        elif lambd != 0:\n","            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n","\n","        \n","        # Update parameters.\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","        # Print the loss every 10000 iterations\n","        if print_cost and i % 10000 == 0:\n","            print(\"Cost after iteration {}: {}\".format(i, cost))\n","        if print_cost and i % 1000 == 0:\n","            costs.append(cost)\n","    \n","    # plot the cost\n","    plt.plot(costs)\n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (x1,000)')\n","    plt.title(\"Learning rate =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"]},{"cell_type":"markdown","metadata":{"id":"ZqGaLPcJc__w"},"source":["#### Look at the cost versus iterations\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ve4K7nxc__w","executionInfo":{"status":"aborted","timestamp":1647396647038,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["parameters_reg = model(X, Y, n_h = 50,learning_rate = 0.01, lambd = 0.001)\n","print (\"On the train set:\")\n","predictions_train = predict_reg(X, Y, parameters_reg)\n","print (\"On the test set:\")\n","predictions_test = predict_reg(Xts, Yts, parameters_reg)"]},{"cell_type":"markdown","metadata":{"id":"bvpOmUr5c__x"},"source":["### ANN - Example classification - use toolbox\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ag1RGoUnc__x"},"source":["#### One Hidden layer ANN - toolbox - regression problem\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WFEMTkqfc__x"},"source":["-   Using toolbox of ANN\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxc8H515c__x","executionInfo":{"status":"aborted","timestamp":1647396647038,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["#import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import math\n","import operator\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# from testCases_v2 import *\n","import sklearn\n","import sklearn.datasets\n","import sklearn.linear_model"]},{"cell_type":"markdown","metadata":{"id":"3oW_Cn28c__y"},"source":["#### Import data and show data and plot\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kiG3au2Pc__y","executionInfo":{"status":"aborted","timestamp":1647396647038,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["data = pd.read_csv('Engine_NOx_classification.csv')\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"3kZ4bWT9c__z"},"source":["##### Training data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVjvG5Jcc__z","executionInfo":{"status":"aborted","timestamp":1647396647039,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["cdf = data[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'NOx [ppm]', 'High NOx']]\n","\n","msk = np.random.rand(len(data)) < 0.8\n","train = cdf[msk]\n","test = cdf[~msk]\n","\n","colors = {0: 'blue', 1:'red', 2:'green', 3:'coral', 4:'orange', 5:'black'}\n","\n","area = 300\n","# area = 200\n","plt.figure(1, figsize=(25, 6))\n","plt.scatter(train['Load [ft.lb]'], train['Engine speed [rpm]'], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"Xy2-q3Clc__z"},"source":["##### Test data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DY5Vc8Yyc__0","executionInfo":{"status":"aborted","timestamp":1647396647039,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["area = 300\n","# area = 200\n","plt.figure(1, figsize=(25, 6))\n","plt.scatter(test['Load [ft.lb]'], test['Engine speed [rpm]'], s=area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)\n","\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"kFiSB01vc__0"},"source":["#### Set Test and training data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MrYiyHF1c__0","executionInfo":{"status":"aborted","timestamp":1647396647039,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import preprocessing\n","import numpy as np\n","\n","# train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","# train_y = np.asanyarray(train[['High NOx']])\n","\n","# test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","# test_y = np.asanyarray(test[['High NOx']])\n","\n","train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]']])\n","train_y = np.asanyarray(train[['High NOx']])\n","\n","test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]']])\n","test_y = np.asanyarray(test[['High NOx']])\n","\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)"]},{"cell_type":"markdown","metadata":{"id":"2A6nakO9c__0"},"source":["#### ANN Classifier\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAvYd32Ic__1","executionInfo":{"status":"aborted","timestamp":1647396647039,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["X = X_train_minmax\n","Y = train_y\n","\n","Xts = X_test_minmax\n","Yts = test_y\n","\n","from sklearn.neural_network import MLPClassifier\n","n_h = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQpdKFfic__1","executionInfo":{"status":"aborted","timestamp":1647396647040,"user_tz":360,"elapsed":12,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(n_h, 1), random_state=1, learning_rate_init = 0.1, activation = 'logistic')\n","clf.fit(X, np.ravel(Y,order='C'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffWyIBYRc__2","executionInfo":{"status":"aborted","timestamp":1647396647040,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["clf.predict(X)"]},{"cell_type":"markdown","metadata":{"id":"znrYmsTfc__2"},"source":["#### Plot for hidden layer size of 5\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTrPgZenc__2","executionInfo":{"status":"aborted","timestamp":1647396647040,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["colors = {0: 'lightcoral', 1:'aqua', 2:'green', 3:'coral', 4:'orange', 5:'black'}\n","X_plot = X_train_minmax.T\n","Y_plot = train_y.T\n","\n","X_plot_ts = X_test_minmax.T\n","Y_plot_ts = test_y.T\n","\n","# Set min and max values and give it some padding\n","x_min, x_max = X_plot[0, :].min() - 0.1, X_plot[0, :].max() + 0.1\n","y_min, y_max = X_plot[1, :].min() - 0.1, X_plot[1, :].max() + 0.1\n","h = 0.01\n","# Generate a grid of points with distance h between them\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","Z = clf.predict(np.c_[xx.ravel().T, yy.ravel().T])\n","Z = Z.reshape(xx.shape)\n","# Plot the contour and training examples\n","plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","area = 30\n","plt.scatter(X_plot[0,:], X_plot[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","plt.scatter(X_plot_ts[0,:], X_plot_ts[1,:], s=8*area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"81-PIdz1c__2"},"source":["#### Show the prediction accuracy\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hb_6bESHc__3","executionInfo":{"status":"aborted","timestamp":1647396647040,"user_tz":360,"elapsed":11,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["predictions = clf.predict(X)\n","predictions_test = clf.predict(Xts)\n","accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.size)*100)\n","accuracy_test = float((np.dot(Yts.T,predictions_test.T) + np.dot(1-Yts.T,1-predictions_test.T))/float(Yts.size)*100)\n","print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n","print (\"Accuracy test for {} hidden units: {} %\".format(n_h, accuracy_test))"]},{"cell_type":"markdown","metadata":{"id":"xWXEl6B-c__3"},"source":["#### Now examine the effect of the hidden layer size\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvAX16J_c__3","executionInfo":{"status":"aborted","timestamp":1647396647041,"user_tz":360,"elapsed":12,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.figure(figsize=(16, 32))\n","hidden_layer_sizes = [5, 10, 15, 20, 30, 50]\n","for i, n_h in enumerate(hidden_layer_sizes):\n","    \n","    plt.subplot(5, 2, i+1)\n","    plt.title('Hidden Layer of size %d' % n_h)\n","    clf = MLPClassifier(solver='lbfgs', max_iter = 1000, alpha=1e-5, hidden_layer_sizes=(n_h, 1), random_state=1, learning_rate_init = 0.1, activation = 'logistic')\n","    clf.fit(X, np.ravel(Y,order='C'))\n","    \n","    \n","\n","    predictions = clf.predict(X)\n","    predictions_test = clf.predict(Xts)\n","    accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.size)*100)\n","    accuracy_test = float((np.dot(Yts.T,predictions_test.T) + np.dot(1-Yts.T,1-predictions_test.T))/float(Yts.size)*100)\n","    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n","    print (\"Accuracy test for {} hidden units: {} %\".format(n_h, accuracy_test))\n","    \n","    X_plot = X_train_minmax.T\n","    Y_plot = train_y.T\n","    \n","    X_plot_ts = X_test_minmax.T\n","    Y_plot_ts = test_y.T\n","    \n","    # Set min and max values and give it some padding\n","    x_min, x_max = X_plot[0, :].min() - 0.1, X_plot[0, :].max() + 0.1\n","    y_min, y_max = X_plot[1, :].min() - 0.1, X_plot[1, :].max() + 0.1\n","    h = 0.01\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","    Z = clf.predict(np.c_[xx.ravel().T, yy.ravel().T])\n","    Z = Z.reshape(xx.shape)\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    area = 30\n","    plt.scatter(X_plot[0,:], X_plot[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","    plt.scatter(X_plot_ts[0,:], X_plot_ts[1,:], s=8*area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)\n","    plt.xlabel('Load [ft.lb]', fontsize=18)\n","    plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"AuAx5nKgc__4"},"source":["#### Examine the effect of regularization (Hidden layer neurons = 15)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMipIyR4c__4","executionInfo":{"status":"aborted","timestamp":1647396647041,"user_tz":360,"elapsed":12,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["n_h = 10\n","plt.figure(figsize=(16, 32))\n","alpha = np.dot(10,[0.001, 0.0001, 0.00001, 0.000001])\n","for i, alp in enumerate(alpha):\n","    \n","    plt.subplot(5, 2, i+1)\n","    plt.title('Regularization %1.6f' %alp)\n","    clf = MLPClassifier(solver='lbfgs', max_iter = 1000, alpha=alp, hidden_layer_sizes=(n_h, 1), random_state=1, learning_rate_init = 0.1, activation = 'logistic')\n","    clf.fit(X, np.ravel(Y,order='C'))\n","    \n","    \n","    predictions = clf.predict(X)\n","    predictions_test = clf.predict(Xts)\n","    accuracy = float((np.dot(Y.T,predictions.T) + np.dot(1-Y.T,1-predictions.T))/float(Y.size)*100)\n","    accuracy_test = float((np.dot(Yts.T,predictions_test.T) + np.dot(1-Yts.T,1-predictions_test.T))/float(Yts.size)*100)\n","    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))\n","    print (\"Accuracy test for {} hidden units: {} %\".format(n_h, accuracy_test))\n","    \n","    X_plot = X_train_minmax.T\n","    Y_plot = train_y.T\n","    \n","    X_plot_ts = X_test_minmax.T\n","    Y_plot_ts = test_y.T\n","    \n","    # Set min and max values and give it some padding\n","    x_min, x_max = X_plot[0, :].min() - 0.1, X_plot[0, :].max() + 0.1\n","    y_min, y_max = X_plot[1, :].min() - 0.1, X_plot[1, :].max() + 0.1\n","    h = 0.01\n","    # Generate a grid of points with distance h between them\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","\n","    Z = clf.predict(np.c_[xx.ravel().T, yy.ravel().T])\n","    Z = Z.reshape(xx.shape)\n","    # Plot the contour and training examples\n","    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n","    area = 30\n","    plt.scatter(X_plot[0,:], X_plot[1,:], s=area, c=np.array(train['High NOx'].map(colors).tolist()), alpha=1)\n","    plt.scatter(X_plot_ts[0,:], X_plot_ts[1,:], s=8*area, c=np.array(test['High NOx'].map(colors).tolist()), alpha=1)\n","    plt.xlabel('Load [ft.lb]', fontsize=18)\n","    plt.ylabel('Engine speed [rpm]', fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"MULZmfGic__5"},"source":["### ANN - Example Regression - one hidden layer - toolbox\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zC0eVI1Ic__5"},"source":["#### One Hidden layer ANN - toolbox - regression problem\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hdkR21esc__5"},"source":["Using toolbox of ANN , import libraries\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zYgaM7rAc__5","executionInfo":{"status":"aborted","timestamp":1647396647041,"user_tz":360,"elapsed":12,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["#import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import math\n","import operator\n","import matplotlib.pyplot as plt\n","get_ipython().run_line_magic('matplotlib', 'inline')\n","\n","\n","# from testCases_v2 import *\n","import sklearn\n","import sklearn.datasets\n","import sklearn.linear_model"]},{"cell_type":"markdown","metadata":{"id":"q35PVv7Wc__6"},"source":["#### Import data and show data and plot\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YMR9Lzic__6","executionInfo":{"status":"aborted","timestamp":1647396647041,"user_tz":360,"elapsed":12,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["data = pd.read_csv('Engine_NOx_classification.csv')\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvkq1wrwc__6","executionInfo":{"status":"aborted","timestamp":1647396647041,"user_tz":360,"elapsed":12,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["cdf = data[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]', 'NOx [ppm]']]\n","\n","msk = np.random.rand(len(data)) < 0.8\n","train = cdf[msk]\n","test = cdf[~msk]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tF5MDpW7c__6","executionInfo":{"status":"aborted","timestamp":1647396647042,"user_tz":360,"elapsed":13,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["font = {'family' : 'normal',\n","        'weight' : 'normal',\n","        'size'   : 18}\n","\n","plt.rc('font', **font)\n","\n","plt.figure(1, figsize=(20, 6))\n","plt.plot(train['Load [ft.lb]'], train['NOx [ppm]'], 'or', label = 'train')\n","plt.plot(test['Load [ft.lb]'], test['NOx [ppm]'], 'ob', label = 'test')\n","plt.xlabel('Load [ft.lb]', fontsize=18)\n","plt.ylabel('NOx [ppm]', fontsize=18)\n","plt.legend(fontsize=18)"]},{"cell_type":"markdown","metadata":{"id":"V5Vd2v-Kc__7"},"source":["#### Set Test and training data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moYQhtfLc__7","executionInfo":{"status":"aborted","timestamp":1647396647042,"user_tz":360,"elapsed":13,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn import preprocessing\n","\n","train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","# train_y = np.asanyarray(train[['High NOx']])\n","\n","test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]','mf [mg/stroke]','Pr [PSI]']])\n","# test_y = np.asanyarray(test[['High NOx']])\n","\n","# train_x = np.asanyarray(train[['Load [ft.lb]','Engine speed [rpm]']])\n","train_y = np.asanyarray(train[['NOx [ppm]']])\n","\n","# test_x = np.asanyarray(test[['Load [ft.lb]','Engine speed [rpm]']])\n","test_y = np.asanyarray(test[['NOx [ppm]']])\n","\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","X_train_minmax = min_max_scaler.fit_transform(train_x)\n","X_test_minmax = min_max_scaler.transform(test_x)"]},{"cell_type":"markdown","metadata":{"id":"OMq2-Kbxc__7"},"source":["#### Train and plot\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xvf0aE9pc__7"},"source":["`sklearn` for NN regression\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CV-MBr06c__8","executionInfo":{"status":"aborted","timestamp":1647396647042,"user_tz":360,"elapsed":13,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["from sklearn.neural_network import MLPRegressor"]},{"cell_type":"markdown","metadata":{"id":"FUaRZYAudAAA"},"source":["-   `solver`  details in next chapter\n","-   `learning_rate_init` called $\\alpha$ in our notes\n","-   `activation` has options\n","-   `learning_rate` can also be changed to decay\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6rdsxAUwdAAA","executionInfo":{"status":"aborted","timestamp":1647396647042,"user_tz":360,"elapsed":12,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["n_h = 15\n","regr_ANN = MLPRegressor(solver='adam', alpha=0.1, learning_rate_init=0.01, max_iter=100000, \n","                        verbose=False, activation = 'tanh',learning_rate= 'constant',\n","                        hidden_layer_sizes=[n_h]).fit(X_train_minmax, train_y.ravel())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVL3FFyXdAAA","executionInfo":{"status":"aborted","timestamp":1647396647043,"user_tz":360,"elapsed":13,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["train_y_hat = regr_ANN.predict(X_train_minmax)\n","test_y_hat = regr_ANN.predict(X_test_minmax)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJhAkUDkdAAB","executionInfo":{"status":"aborted","timestamp":1647396647043,"user_tz":360,"elapsed":13,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["plt.figure(1, figsize=(6, 6))\n","plt.plot([100,350], [100, 350],  '-k')\n","plt.plot(train_y_hat, train['NOx [ppm]'], 'ob', label = 'train')\n","plt.plot(test_y_hat, test['NOx [ppm]'], 'or', label = 'test')\n","plt.ylabel(\"Exp NOx [ppm]\", fontsize=18)\n","plt.xlabel(\"Pred NOx [ppm]\", fontsize=18)\n","plt.legend(fontsize=18)"]},{"cell_type":"markdown","metadata":{"id":"WVvtlZyzdAAB"},"source":["#### Check R-squared\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11Ka5vl6dAAB","executionInfo":{"status":"aborted","timestamp":1647396647043,"user_tz":360,"elapsed":3418,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["r2_test = regr_ANN.score(X_test_minmax, test_y.ravel())\n","r2_train = regr_ANN.score(X_train_minmax, train_y.ravel())\n","\n","print ('Train R^2= %1.3f' % r2_train)\n","print ('Test R^2= %1.3f' % r2_test)"]},{"cell_type":"markdown","metadata":{"id":"KY-iLrzvdAAC"},"source":["#### Examine the effect of number of neurons in a hidden layer and plot\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggM232PEdAAC","executionInfo":{"status":"aborted","timestamp":1647396647043,"user_tz":360,"elapsed":3414,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["font = {'family' : 'normal',\n","        'size'   : 10}\n","plt.rc('font', **font)\n","\n","plt.figure(figsize=(16, 32))\n","hidden_layer_sizes = [1, 2, 5, 10, 20, 40]\n","for i, n_h in enumerate(hidden_layer_sizes):\n","    \n","    plt.subplot(5, 2, i+1)\n","    plt.title('Hidden Layer of size %d' % n_h)\n","    regr_ANN = MLPRegressor(solver='adam', alpha=0.1, learning_rate_init=0.01, max_iter=100000, \n","                        verbose=False, activation = 'tanh',learning_rate= 'constant',\n","                        hidden_layer_sizes=[n_h]).fit(X_train_minmax, train_y.ravel())\n","    \n","    \n","    train_y_hat = regr_ANN.predict(X_train_minmax)\n","    test_y_hat = regr_ANN.predict(X_test_minmax)\n","    plt.plot([100,350], [100, 350],  '-k')\n","    plt.plot(train_y_hat, train['NOx [ppm]'], 'ob', label = 'train')\n","    plt.plot(test_y_hat, test['NOx [ppm]'], 'or', label = 'test')\n","    plt.ylabel(\"Exp NOx [ppm]\", fontsize=10)\n","    plt.xlabel(\"Pred NOx [ppm]\", fontsize=10)\n","    plt.legend(fontsize=10)\n","    plt.gca().set_aspect('equal', adjustable='box')\n","    r2_test = regr_ANN.score(X_test_minmax, test_y.ravel())\n","    r2_train = regr_ANN.score(X_train_minmax, train_y.ravel())\n","\n","    print (\"R^2 train for {} hidden units: {} \".format(n_h, r2_train))\n","    print (\"R^2 test for {} hidden units: {} \".format(n_h, r2_test))"]},{"cell_type":"markdown","metadata":{"id":"VhZBSHhVdAAC"},"source":["#### Examine the effect of regularization (Hidden layer neurons = 20)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5dUakqFdAAD","executionInfo":{"status":"aborted","timestamp":1647396647044,"user_tz":360,"elapsed":3412,"user":{"displayName":"Armin Norouzi Yengeje","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwPSxnIqqwRUaXyyszLRr-3pk9eNTrn6EvdvkkWk=s64","userId":"07700221665224727665"}}},"outputs":[],"source":["n_h = 20\n","font = {'family' : 'normal',\n","        'size'   : 10}\n","\n","plt.rc('font', **font)\n","\n","plt.figure(figsize=(16, 32))\n","alpha = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n","for i, alp in enumerate(alpha):\n","    \n","    plt.subplot(5, 2, i+1)\n","    plt.title('regularization  %1.6f' % alp)\n","    regr_ANN = MLPRegressor(solver='adam', alpha=alp, learning_rate_init=0.01, max_iter=100000, \n","                        verbose=False, activation = 'tanh',learning_rate= 'constant',\n","                        hidden_layer_sizes=[n_h]).fit(X_train_minmax, train_y.ravel())\n","    \n","    \n","    train_y_hat = regr_ANN.predict(X_train_minmax)\n","    test_y_hat = regr_ANN.predict(X_test_minmax)\n","    plt.plot([100,350], [100, 350],  '-k')\n","    plt.plot(train_y_hat, train['NOx [ppm]'], 'ob', label = 'train')\n","    plt.plot(test_y_hat, test['NOx [ppm]'], 'or', label = 'test')\n","    plt.ylabel(\"Exp NOx [ppm]\", fontsize=10)\n","    plt.xlabel(\"Pred NOx [ppm]\", fontsize=10)\n","    plt.legend(fontsize=10)\n","    plt.gca().set_aspect('equal', adjustable='box')\n","    r2_test = regr_ANN.score(X_test_minmax, test_y.ravel())\n","    r2_train = regr_ANN.score(X_train_minmax, train_y.ravel())\n","\n","    print (\"R^2 train for {} regularization : {} \".format(alp, r2_train))\n","    print (\"R^2 test for {} regularization : {} \".format(alp, r2_test))"]},{"cell_type":"markdown","metadata":{"id":"V9eoWat5dAAD"},"source":["## References\n","\n","[1] Mohamad H Hassoun et al. Fundamentals of arti\u001ccial neural networks. MIT press, 1995.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EYgRYUdCdAAD"},"source":["\\printbibliography\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"org":null,"colab":{"name":"ANN_v2022_1.ipynb","provenance":[],"collapsed_sections":["MtXR4XoVc_-9","-tQRn0Otc_-_","7a_kdNjxc__B","MsB1GPtZc__C","agMSWXHlc__D","YTjtno1sc__E","-mjqR-Ssc__E","zHlP3S5Zc__F","U6W7_Yb7c__G","phXfWSlnc__H","bSUNwMcQc__I","l584WeCTc__J","ve59sVWBc__J","iFPnE9eoc__K","Y-Wp2hG8c__K","N48c3N-Zc__L","C6nNvHPsc__L","5BVGRPf3c__M","obuQ6JZZc__N","ObzXe92kc__N","IggPwRq6c__O","crCCQO1Xc__P","UoaCeh0wc__Q","K9z5OVFuc__Q","6SEymIhyc__S","FNUv57kbc__U","IqNmUZJlc__W","8dD5fiLPc__W","nleJX-e3c__X","Bnf3w9T4c__Y","BPj4-hAjc__Z","wWDhCaJrc__b","Eh6zMrIPc__b","Kb6-dZ9Lc__d","g0YaVi8lc__d","pdLkbWgac__e","UjBw28Eec__f","rB-95DIKc__g","1mZlxoWpc__h","3q6CLckQc__j","16us5V50c__k","bi85h-Mfc__m","15w5RwjTc__o","SUP8u9npc__p","nDivN5tqc__q","v_R5JlBNc__q","LRkIhDU_c__r","FC8i1amsc__s","sQmOY6wyc__t","cmXOyP1xc__u","BLzpbRQJc__v","pdYgGcUlc__v","ZqGaLPcJc__w","bvpOmUr5c__x","Ag1RGoUnc__x","3oW_Cn28c__y","kFiSB01vc__0","2A6nakO9c__0","znrYmsTfc__2","81-PIdz1c__2","xWXEl6B-c__3","AuAx5nKgc__4","MULZmfGic__5","zC0eVI1Ic__5","q35PVv7Wc__6","V5Vd2v-Kc__7","OMq2-Kbxc__7","WVvtlZyzdAAB","KY-iLrzvdAAC","VhZBSHhVdAAC","V9eoWat5dAAD"]}},"nbformat":4,"nbformat_minor":0}